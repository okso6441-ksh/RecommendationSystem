2001_IB_CFR [Item-Based Collaborative Filtering Recommendation]

![main](./image/main.PNG)

---

ABSTRACT
Recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for information, products or services during a live interaction. 

These systems, especially the k-nearest neighbor collaborative filtering based ones, are achieving widespread success on the Web. 

The tremendous growth in the amount of available information and the number of visitors to Web sites in recent years poses some key challenges for recommender systems. 

These are: producing high quality recommendations, performing many recommendations per second for millions of users and items and achieving high coverage in the face of data sparsity. 

In traditional collaborative filtering systems the amount of work increases with the number of participants in the system. 

New recommender system technologies are needed that can quickly produce high quality recommendations, even for very large-scale problems. 

To address these issues we have explored item-based collaborative filtering techniques. 

Item-based techniques first analyze the user-item matrix to identify relationships between different items, and then use these relationships to indirectly compute recommendations for users.


ìš”ì•½
Recommender ì‹œìŠ¤í…œì€ ì‹¤ì‹œê°„ ìƒí˜¸ ì‘ìš© ì¤‘ì— ì •ë³´, ì œí’ˆ ë˜ëŠ” ì„œë¹„ìŠ¤ì— ëŒ€í•œ ê°œì¸í™” ëœ ê¶Œì¥ ì‚¬í•­ì„ ë§Œë“œëŠ” ë¬¸ì œì— ì§€ì‹ ê²€ìƒ‰ ê¸°ìˆ ì„ ì ìš©í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì‹œìŠ¤í…œ, íŠ¹íˆ k- ìµœê·¼ ì ‘ ì´ì›ƒ í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ì‹œìŠ¤í…œì€ ì›¹ì—ì„œ ê´‘ë²”ìœ„í•œ ì„±ê³µì„ ê±°ë‘ê³  ìˆìŠµë‹ˆë‹¤.

ìµœê·¼ ëª‡ ë…„ ë™ì•ˆ ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ì˜ ì–‘ê³¼ ì›¹ ì‚¬ì´íŠ¸ ë°©ë¬¸ì ìˆ˜ê°€ ì—„ì²­ë‚˜ê²Œ ì¦ê°€í•¨ì— ë”°ë¼ ì¶”ì²œ ì‹œìŠ¤í…œì— ëª‡ ê°€ì§€ ì£¼ìš” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

ì—¬ê¸°ì—ëŠ” ê³ í’ˆì§ˆ ê¶Œì¥ ì‚¬í•­ ìƒì„±, ìˆ˜ë°±ë§Œ ëª…ì˜ ì‚¬ìš©ì ë° í•­ëª©ì— ëŒ€í•´ ì´ˆë‹¹ ë§ì€ ê¶Œì¥ ì‚¬í•­ ìˆ˜í–‰, ë°ì´í„° í¬ì†Œì„±ì— ì§ë©´ í•œ ë†’ì€ ì ìš© ë²”ìœ„ ë‹¬ì„± ë“±ì´ ìˆìŠµë‹ˆë‹¤.

ê¸°ì¡´ì˜ í˜‘ì—… í•„í„°ë§ ì‹œìŠ¤í…œì—ì„œ ì‘ì—…ëŸ‰ì€ ì‹œìŠ¤í…œ ì°¸ì—¬ì ìˆ˜ì— ë”°ë¼ ì¦ê°€í•©ë‹ˆë‹¤.

ë§¤ìš° í° ê·œëª¨ì˜ ë¬¸ì œì—ì„œë„ ê³ í’ˆì§ˆ ì¶”ì²œì„ ì‹ ì†í•˜ê²Œ ìƒì„± í•  ìˆ˜ìˆëŠ” ìƒˆë¡œìš´ ì¶”ì²œ ì‹œìŠ¤í…œ ê¸°ìˆ ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•­ëª© ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ê¸°ìˆ ì„ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.

í•­ëª© ê¸°ë°˜ ê¸°ìˆ ì€ ë¨¼ì € ì‚¬ìš©ì í•­ëª© ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ í•­ëª© ê°„ì˜ ê´€ê³„ë¥¼ ì‹ë³„ í•œ ë‹¤ìŒ ì´ëŸ¬í•œ ê´€ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì— ëŒ€í•œ ê¶Œì¥ ì‚¬í•­ì„ ê°„ì ‘ì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.





In this paper we analyze different item-based recommendation generation algorithms. 

We look into different techniques for computing item-item similarities (e.g., item-item correlation vs. cosine similarities between item vectors) and different techniques for obtaining recommendations from them (e.g., weighted sum vs. regression model). 

Finally, we experimentally evaluate our results and compare them to the basic k-nearest neighbor approach. 

Our experiments suggest that item-based algorithms provide dramatically better performance than user-based algorithms, while at the same time providing better quality than the best available userbased algorithms.

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ í•­ëª© ê¸°ë°˜ ì¶”ì²œ ìƒì„± ì•Œê³ ë¦¬ì¦˜ì„ ë¶„ì„í•©ë‹ˆë‹¤.

í•­ëª©-í•­ëª© ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ê¸°ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ìˆ  (ì˜ˆ : í•­ëª©-í•­ëª© ìƒê´€ ëŒ€ í•­ëª© ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±)ê³¼ ê¶Œì¥ ì‚¬í•­ì„ ì–»ê¸°ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ìˆ  (ì˜ˆ : ê°€ì¤‘ì¹˜ í•©ê³„ ëŒ€ íšŒê·€ ëª¨ë¸)ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ê¸°ë³¸ k- ìµœê·¼ ì ‘ ì´ì›ƒ ì ‘ê·¼ë²•ê³¼ ë¹„êµí•©ë‹ˆë‹¤.

ìš°ë¦¬ì˜ ì‹¤í—˜ì— ë”°ë¥´ë©´ í•­ëª© ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ í›¨ì”¬ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” ë™ì‹œì— ì‚¬ìš© ê°€ëŠ¥í•œ ìµœìƒì˜ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ë‚˜ì€ í’ˆì§ˆì„ ì œê³µí•©ë‹ˆë‹¤.








1. INTRODUCTION

The amount of information in the world is increasing far more quickly than our ability to process it. 

All of us have known the feeling of being overwhelmed by the number of new books, journal articles, and conference proceedings coming out each year. 

Technology has dramatically reduced the barriers to publishing and distributing information. 

Now it is time to create the technologies that can help us sift through all the available information to find that which is most valuable to us.

One of the most promising such technologies is col laborative filtering [19, 27, 14, 16]. 

Collaborative filtering works by building a database of preferences for items by users. 

A new user, Neo, is matched against the database to discover neighbors, which are other users who have historically had similar taste to Neo. 

Items that the neighbors like are then recommended to Neo, as he will probably also like them. 

Collaborative filtering has been very successful in both research and practice, and in both information filtering applications and E-commerce applications. 

However, there remain important research questions in overcoming two fundamental challenges for collaborative filtering recommender systems. 

The first challenge is to improve the scalability of the collaborative filtering algorithms. 

These algorithms are able to search tens of thousands of potential neighbors in real-time, but the demands of modern systems are to search tens of millions of potential neighbors. 

Further, existing algorithms have performance problems with individual users for whom the site has large amounts of information. 

For instance, if a site is using browsing patterns as indications of content preference, it may have thousands of data points for its most frequent visitors. 

These "long user rows" slow down the number of neighbors that can be searched per second, further reducing scalability. 

The second challenge is to improve the quality of the recommendations for the users. 

Users need recommendations they can trust to help them find items they will like. 

ì„¸ê³„ì˜ ì •ë³´ëŸ‰ì€ ìš°ë¦¬ê°€ ì²˜ë¦¬í•˜ëŠ” ëŠ¥ë ¥ë³´ë‹¤ í›¨ì”¬ ë” ë¹ ë¥´ê²Œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ ëª¨ë‘ëŠ” ë§¤ë…„ ë‚˜ì˜¤ëŠ” ìƒˆë¡œìš´ ì±…, ì €ë„ ê¸°ì‚¬ ë° íšŒì˜ ì ˆì°¨ì— ì••ë„ë‹¹í•˜ëŠ” ëŠë‚Œì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤.

ê¸°ìˆ ì€ ì •ë³´ ê²Œì‹œ ë° ë°°í¬ì˜ ì¥ë²½ì„ ê·¹ì ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

ì´ì œ ìš°ë¦¬ì—ê²Œ ê°€ì¥ ê°€ì¹˜ìˆëŠ” ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ ì‚´í´ë³¼ ìˆ˜ìˆëŠ” ê¸°ìˆ ì„ ë§Œë“¤ ë•Œì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ê°€ì¥ ìœ ë§í•œ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ëŠ” í˜‘ì—… í•„í„°ë§ì…ë‹ˆë‹¤ [19, 27, 14, 16].

í˜‘ì—… í•„í„°ë§ì€ ì‚¬ìš©ìê°€ í•­ëª©ì— ëŒ€í•œ ì„ í˜¸ë„ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì—¬ ì‘ë™í•©ë‹ˆë‹¤.

ìƒˆë¡œìš´ ì‚¬ìš©ì ì¸ NeoëŠ” ì´ì „ì— Neoì™€ ë¹„ìŠ·í•œ ì·¨í–¥ì„ ê°€ì§„ ë‹¤ë¥¸ ì‚¬ìš©ì ì¸ ì´ì›ƒì„ ì°¾ê¸° ìœ„í•´ ë°ì´í„°ë² ì´ìŠ¤ì™€ ëŒ€ì¡°ë©ë‹ˆë‹¤.

ì´ì›ƒ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í•˜ëŠ” ì•„ì´í…œì€ ì•„ë§ˆ ì¢‹ì•„í•  ê²ƒì´ë¯€ë¡œ Neoì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤.

í˜‘ì—… í•„í„°ë§ì€ ì—°êµ¬ì™€ ì‹¤ìŠµ, ì •ë³´ í•„í„°ë§ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì „ì ìƒê±°ë˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‘ì—ì„œ ë§¤ìš° ì„±ê³µì ì´ì—ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ í˜‘ì—… í•„í„°ë§ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ë‘ ê°€ì§€ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ëŠ” ë°ìˆì–´ ì¤‘ìš”í•œ ì—°êµ¬ ì§ˆë¬¸ì´ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤.

ì²« ë²ˆì§¸ ê³¼ì œëŠ” í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì˜ í™•ì¥ ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì€ ìˆ˜ë§Œ ê°œì˜ ì ì¬ì  ì¸ ì´ì›ƒì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰ í•  ìˆ˜ ìˆì§€ë§Œ í˜„ëŒ€ ì‹œìŠ¤í…œì˜ ìš”êµ¬ ì‚¬í•­ì€ ìˆ˜ì²œë§Œ ê°œì˜ ì ì¬ì  ì¸ ì´ì›ƒì„ ê²€ìƒ‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë˜í•œ ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ì€ ì‚¬ì´íŠ¸ì— ë§ì€ ì–‘ì˜ ì •ë³´ê°€ìˆëŠ” ê°œë³„ ì‚¬ìš©ìì—ê²Œ ì„±ëŠ¥ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰ íŒ¨í„´ì„ ì½˜í…ì¸  ì„ í˜¸ë„ì˜ ì§€í‘œë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ê°€ì¥ ë¹ˆë²ˆí•œ ë°©ë¬¸ìì— ëŒ€í•œ ìˆ˜ì²œ ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ "ê¸´ ì‚¬ìš©ì í–‰"ì€ ì´ˆë‹¹ ê²€ìƒ‰ í•  ìˆ˜ìˆëŠ” ì¸ì ‘ í•­ëª© ìˆ˜ë¥¼ ì¤„ì—¬ í™•ì¥ ì„±ì„ ë”ìš± ê°ì†Œì‹œí‚µë‹ˆë‹¤.

ë‘ ë²ˆì§¸ ê³¼ì œëŠ” ì‚¬ìš©ìë¥¼ìœ„í•œ ê¶Œì¥ ì‚¬í•­ì˜ í’ˆì§ˆì„ ê°œì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì‚¬ìš©ìëŠ” ì¢‹ì•„í•  í•­ëª©ì„ ì°¾ëŠ” ë° ë„ì›€ì´ë˜ë„ë¡ ì‹ ë¢°í•  ìˆ˜ìˆëŠ” ê¶Œì¥ ì‚¬í•­ì´ í•„ìš”í•©ë‹ˆë‹¤.







Users will "vote with their feet" by refusing to use recommender systems that are not consistently accurate for them.

In some ways these two challenges are in conflict, since the less time an algorithm spends searching for neighbors, the more scalable it will be, and the worse its quality. 

For this reason, it is important to treat the two challenges simultaneously so the solutions discovered are both useful and practical.

In this paper, we address these issues of recommender systems by applying a different approach-item-based algorithm. 

The bottleneck in conventional collaborative filtering algorithms is the search for neighbors among a large user population of potential neighbors [12]. 

Item-based algorithms avoid this bottleneck by exploring the relationships between items first, rather than the relationships between users. 

Recommendations for users are computed by finding items that are similar to other items the user has liked. 

Because the relationships between items are relatively static, item-based algorithms may be able to provide the same quality as the user-based algorithms with less online computation.

1.1 Related Work
In this section we briefly present some of the research literature related to collaborative filtering, recommender systems, data mining and personalization.

Tapestry [10] is one of the earliest implementations of collaborative filtering-based recommender systems. 

This system relied on the explicit opinions of people from a close-knit community, such as an oce workgroup. 

However, recommender system for large communities cannot depend on each person knowing the others. 

ì‚¬ìš©ìëŠ” ì§€ì†ì ìœ¼ë¡œ ì •í™•í•˜ì§€ ì•Šì€ ì¶”ì²œ ì‹œìŠ¤í…œ ì‚¬ìš©ì„ ê±°ë¶€í•¨ìœ¼ë¡œì¨ "ë°œë¡œ íˆ¬í‘œ"í•©ë‹ˆë‹¤.

ì•Œê³ ë¦¬ì¦˜ì´ ì´ì›ƒì„ ê²€ìƒ‰í•˜ëŠ” ë° ì†Œìš”ë˜ëŠ” ì‹œê°„ì´ ì ì„ìˆ˜ë¡ í™•ì¥ ì„±ì´ ë†’ì•„ì§€ê³  í’ˆì§ˆì´ ì €í•˜ë˜ê¸° ë•Œë¬¸ì— ì–´ë–¤ë©´ì—ì„œëŠ”ì´ ë‘ ê°€ì§€ ë¬¸ì œê°€ ì¶©ëŒí•©ë‹ˆë‹¤.

ë”°ë¼ì„œ ë°œê²¬ ëœ ì†”ë£¨ì…˜ì´ ìœ ìš©í•˜ê³  ì‹¤ìš©ì  ì´ë„ë¡ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

ì´ ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ë²•-í•­ëª© ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

ê¸°ì¡´ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì˜ ë³‘ëª© í˜„ìƒì€ ì ì¬ì  ì¸ ì´ì›ƒì˜ ëŒ€ê·œëª¨ ì‚¬ìš©ì ì§‘ë‹¨ ì¤‘ì—ì„œ ì´ì›ƒì„ ê²€ìƒ‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤ [12].

í•­ëª© ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ì‚¬ìš©ì ê°„ì˜ ê´€ê³„ê°€ ì•„ë‹Œ í•­ëª© ê°„ì˜ ê´€ê³„ë¥¼ ë¨¼ì € íƒìƒ‰í•˜ì—¬ ì´ëŸ¬í•œ ë³‘ëª© í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

ì‚¬ìš©ìì— ëŒ€í•œ ì¶”ì²œì€ ì‚¬ìš©ìê°€ ì¢‹ì•„ í•œ ë‹¤ë¥¸ í•­ëª©ê³¼ ìœ ì‚¬í•œ í•­ëª©ì„ ì°¾ì•„ ê³„ì‚°ë©ë‹ˆë‹¤.

í•­ëª© ê°„ì˜ ê´€ê³„ê°€ ë¹„êµì  ì •ì ì´ê¸° ë•Œë¬¸ì— í•­ëª© ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ì ì€ ì˜¨ë¼ì¸ ê³„ì‚°ìœ¼ë¡œ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ê³¼ ë™ì¼í•œ í’ˆì§ˆì„ ì œê³µ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1.1 ê´€ë ¨ ì‘ì—…
ì´ ì„¹ì…˜ì—ì„œëŠ” í˜‘ì—… í•„í„°ë§, ì¶”ì²œ ì‹œìŠ¤í…œ, ë°ì´í„° ë§ˆì´ë‹ ë° ê°œì¸í™”ì™€ ê´€ë ¨ëœ ëª‡ ê°€ì§€ ì—°êµ¬ ë¬¸í—Œì„ ê°„ëµí•˜ê²Œ ì†Œê°œí•©ë‹ˆë‹¤.

Tapestry [10]ëŠ” í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì´ˆê¸° êµ¬í˜„ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

ì´ ì‹œìŠ¤í…œì€ ì‚¬ë¬´ì‹¤ ì‘ì—… ê·¸ë£¹ê³¼ ê°™ì€ ê¸´ë°€í•œ ì»¤ë®¤ë‹ˆí‹°ì˜ ì‚¬ëŒë“¤ì˜ ëª…ì‹œì ì¸ ì˜ê²¬ì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ë¥¼ìœ„í•œ ì¶”ì²œ ì‹œìŠ¤í…œì€ ë‹¤ë¥¸ ì‚¬ëŒì„ ì•„ëŠ” ì‚¬ëŒ ê°ìì—ê²Œ ì˜ì¡´ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.






Later, several ratings-based automated recommender systems were developed. 

The GroupLens research system [19, 16] provides a pseudonymous collaborative filtering solution for Usenet news and movies. 

Ringo [27] and Video Recommender [14] are email and webbased systems that generate recommendations on music and movies, respectively. 

A special issue of Communications of the ACM [20] presents a number of different recommender systems.

Other technologies have also been applied to recommender systems, including Bayesian networks, clustering, and Horting. 

Bayesian networks create a model based on a training set with a decision tree at each node and edges representing user information. 

The model can be built off-line over a matter of hours or days. 

The resulting model is very small, very fast, and essentially as accurate as nearest neighbor methods [6]. 

Bayesian networks may prove practical for environments in which knowledge of user preferences changes slowly with respect to the time needed to build the model but are not suitable for environments in which user preference models must be updated rapidly or frequently. 

Clustering techniques work by identifying groups of users who appear to have similar preferences. 

Once the clusters are created, predictions for an individual can be made by averaging the opinions of the other users in that cluster. 

Some clustering techniques represent each user with partial participation in several clusters. 

The prediction is then an average across the clusters, weighted by degree of participation.

Clustering techniques usually produce less-personal recommendations than other methods, and in some cases, the clusters have worse accuracy than nearest neighbor algorithms [6]. 

Once the clustering is complete, however, performance can be very good, since the size of the group that must be analyzed is much smaller. 

Clustering techniques can also be applied as a "first step" for shrinking the candidate set in a nearest neighbor algorithm or for distributing nearestneighbor computation across several recommender engines. 

ë‚˜ì¤‘ì— ì—¬ëŸ¬ ë“±ê¸‰ ê¸°ë°˜ ìë™ ì¶”ì²œ ì‹œìŠ¤í…œì´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.

GroupLens ì—°êµ¬ ì‹œìŠ¤í…œ [19, 16]ì€ Usenet ë‰´ìŠ¤ ë° ì˜í™”ì— ëŒ€í•œ ìµëª…ì˜ í˜‘ì—… í•„í„°ë§ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

Ringo [27]ì™€ Video Recommender [14]ëŠ” ê°ê° ìŒì•…ê³¼ ì˜í™”ì— ëŒ€í•œ ì¶”ì²œì„ ìƒì„±í•˜ëŠ” ì´ë©”ì¼ ë° ì›¹ ê¸°ë°˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

Communications of the ACM [20]ì˜ íŠ¹ë³„ í˜¸ëŠ” ë‹¤ì–‘í•œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì œì‹œí•©ë‹ˆë‹¤.

Bayesian ë„¤íŠ¸ì›Œí¬, í´ëŸ¬ìŠ¤í„°ë§ ë° Hortingì„ í¬í•¨í•œ ë‹¤ë¥¸ ê¸°ìˆ ë„ ì¶”ì²œ ì‹œìŠ¤í…œì— ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ëŠ” ê° ë…¸ë“œì˜ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ì™€ ì‚¬ìš©ì ì •ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—ì§€ê°€ìˆëŠ” í›ˆë ¨ ì„¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.

ì´ ëª¨ë¸ì€ ëª‡ ì‹œê°„ ë˜ëŠ” ë©°ì¹  ë™ì•ˆ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ êµ¬ì¶• í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê²°ê³¼ ëª¨ë¸ì€ ë§¤ìš° ì‘ê³  ë¹ ë¥´ë©° ê¸°ë³¸ì ìœ¼ë¡œ ìµœê·¼ ì ‘ ì´ì›ƒ ë°©ë²•ë§Œí¼ ì •í™•í•©ë‹ˆë‹¤ [6].

ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë° í•„ìš”í•œ ì‹œê°„ê³¼ ê´€ë ¨í•˜ì—¬ ì‚¬ìš©ì ì„ í˜¸ë„ì— ëŒ€í•œ ì§€ì‹ì´ ëŠë¦¬ê²Œ ë³€í•˜ëŠ” í™˜ê²½ì—ì„œ ì‹¤ìš©ì  ì¼ ìˆ˜ ìˆì§€ë§Œ ì‚¬ìš©ì ì„ í˜¸ë„ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ë˜ëŠ” ìì£¼ ì—…ë°ì´íŠ¸í•´ì•¼í•˜ëŠ” í™˜ê²½ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

í´ëŸ¬ìŠ¤í„°ë§ ê¸°ìˆ ì€ ë¹„ìŠ·í•œ ì„ í˜¸ë„ë¥¼ ê°€ì§„ ê²ƒìœ¼ë¡œ ë³´ì´ëŠ” ì‚¬ìš©ì ê·¸ë£¹ì„ ì‹ë³„í•˜ì—¬ ì‘ë™í•©ë‹ˆë‹¤.

í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ë©´ í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì—ìˆëŠ” ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ì˜ê²¬ì„ í‰ê· í•˜ì—¬ ê°œì¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ìˆ ì€ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ì— ë¶€ë¶„ì ìœ¼ë¡œ ì°¸ì—¬í•˜ëŠ” ê° ì‚¬ìš©ìë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ ì˜ˆì¸¡ì€ ì°¸ì—¬ ì •ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ ëœ í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ í‰ê· ì…ë‹ˆë‹¤.

í´ëŸ¬ìŠ¤í„°ë§ ê¸°ìˆ ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ë¥¸ ë°©ë²•ë³´ë‹¤ ëœ ê°œì¸ì ì¸ ê¶Œì¥ ì‚¬í•­ì„ ìƒì„±í•˜ë©° ê²½ìš°ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„°ëŠ” ìµœê·¼ ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ì •í™•ë„ê°€ ë–¨ì–´ì§‘ë‹ˆë‹¤ [6].

ê·¸ëŸ¬ë‚˜ í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ë©´ ë¶„ì„í•´ì•¼í•˜ëŠ” ê·¸ë£¹ì˜ í¬ê¸°ê°€ í›¨ì”¬ ë” ì‘ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í´ëŸ¬ìŠ¤í„°ë§ ê¸°ìˆ ì€ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì—ì„œ í›„ë³´ ì„¸íŠ¸ë¥¼ ì¶•ì†Œí•˜ê±°ë‚˜ ì—¬ëŸ¬ ì¶”ì²œ ì—”ì§„ì— ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ê³„ì‚°ì„ ë°°í¬í•˜ê¸°ìœ„í•œ "ì²« ë²ˆì§¸ ë‹¨ê³„"ë¡œ ì ìš©ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.






While dividing the population into clusters may hurt the accuracy or recommendations to users near the fringes of their assigned cluster, pre-clustering may be a worthwhile trade-off between accuracy and throughput.

Horting is a graph-based technique in which nodes are users, and edges between nodes indicate degree of similarity between two users [1]. 

Predictions are produced by walking the graph to nearby nodes and combining the opinions of the nearby users. 

Horting differs from nearest neighbor as the graph may be walked through other users who have not rated the item in question, thus exploring transitive relationships that nearest neighbor algorithms do not consider.

In one study using synthetic data, Horting produced better predictions than a nearest neighbor algorithm [1].

Schafer et al., [26] present a detailed taxonomy and examples of recommender systems used in E-commerce and how they can provide one-to-one personalization and at the same can capture customer loyalty. 

Although these systems have been successful in the past, their widespread use has exposed some of their limitations such as the problems of sparsity in the data set, problems associated with high dimensionality and so on. 

Sparsity problem in recommender system has been addressed in [23, 11]. 

The problems associated with high dimensionality in recommender systems have been discussed in [4], and application of dimensionality reduction techniques to address these issues has been investigated in [24].

Our work explores the extent to which item-based recommenders, a new class of recommender algorithms, are able to solve these problems.

ëª¨ì§‘ë‹¨ì„ í´ëŸ¬ìŠ¤í„°ë¡œ ë‚˜ëˆ„ë©´ í• ë‹¹ ëœ í´ëŸ¬ìŠ¤í„° ì£¼ë³€ì—ìˆëŠ” ì‚¬ìš©ìì— ëŒ€í•œ ì •í™•ì„±ì´ë‚˜ ê¶Œì¥ ì‚¬í•­ì´ ì†ìƒ ë  ìˆ˜ ìˆì§€ë§Œ ì‚¬ì „ í´ëŸ¬ìŠ¤í„°ë§ì€ ì •í™•ë„ì™€ ì²˜ë¦¬ëŸ‰ ì‚¬ì´ì˜ ì ì ˆí•œ ì ˆì¶©ì•ˆì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Hortingì€ ë…¸ë“œê°€ ì‚¬ìš©ìì´ê³  ë…¸ë“œ ì‚¬ì´ì˜ ê²½ê³„ëŠ” ë‘ ì‚¬ìš©ì ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ ê¸°ë°˜ ê¸°ìˆ ì…ë‹ˆë‹¤ [1].

ê·¸ë˜í”„ë¥¼ ì£¼ë³€ ë…¸ë“œë¡œ ì´ë™í•˜ê³  ì£¼ë³€ ì‚¬ìš©ìì˜ ì˜ê²¬ì„ ê²°í•©í•˜ì—¬ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.

Hortingì€ í•´ë‹¹ í•­ëª©ì„ í‰ê°€í•˜ì§€ ì•Šì€ ë‹¤ë¥¸ ì‚¬ìš©ìë¥¼ í†µí•´ ê·¸ë˜í”„ê°€ í‘œì‹œ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ë”°ë¼ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì´ ê³ ë ¤í•˜ì§€ ì•ŠëŠ” ì „ ì´ì  ê´€ê³„ë¥¼ íƒìƒ‰ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•©ì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í•œ ì—°êµ¬ì—ì„œ Hortingì€ ìµœê·¼ ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ë‚˜ì€ ì˜ˆì¸¡ì„ ë‚´ë†“ì•˜ìŠµë‹ˆë‹¤ [1].

Schafer et al., [26]ì€ ì „ì ìƒê±°ë˜ì— ì‚¬ìš©ë˜ëŠ” ì¶”ì²œ ì‹œìŠ¤í…œì˜ ìì„¸í•œ ë¶„ë¥˜ë²•ê³¼ ì˜ˆë¥¼ ì œì‹œí•˜ê³  ì´ë“¤ì´ ì¼ëŒ€ì¼ ê°œì¸í™”ë¥¼ ì œê³µí•˜ê³  ë™ì‹œì— ê³ ê° ì¶©ì„±ë„ë¥¼ í¬ì°© í•  ìˆ˜ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì‹œìŠ¤í…œì€ ê³¼ê±°ì—ëŠ” ì„±ê³µì  ì´ì—ˆì§€ë§Œ ë„ë¦¬ ì‚¬ìš©ë˜ë©´ì„œ ë°ì´í„° ì„¸íŠ¸ì˜ í¬ì†Œì„± ë¬¸ì œ, ë†’ì€ ì°¨ì›ê³¼ ê´€ë ¨ëœ ë¬¸ì œ ë“±ê³¼ ê°™ì€ ëª‡ ê°€ì§€ í•œê³„ê°€ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.

ì¶”ì²œ ì‹œìŠ¤í…œì˜ í¬ì†Œì„± ë¬¸ì œëŠ” [23, 11]ì—ì„œ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.

ì¶”ì²œ ì‹œìŠ¤í…œì˜ ê³ ì°¨ì› ì„±ê³¼ ê´€ë ¨ëœ ë¬¸ì œëŠ” [4]ì—ì„œ ë…¼ì˜ë˜ì—ˆìœ¼ë©°, ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ìœ„í•œ ì°¨ì› ì¶•ì†Œ ê¸°ìˆ ì˜ ì ìš©ì€ [24]ì—ì„œ ì¡°ì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ìƒˆë¡œìš´ ì¢…ë¥˜ì˜ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ì¸ í•­ëª© ê¸°ë°˜ ì¶”ì²œìê°€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ìˆëŠ” ì •ë„ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤.




1.2 Contributions
This paper has three primary research contributions:

1. Analysis of the item-based prediction algorithms and identification of different ways to implement its subtasks.

2. Formulation of a precomputed model of item similarity to increase the online scalability of item-based recommendations.

3. An experimental comparison of the quality of several different item-based algorithms to the classic user-based (nearest neighbor) algorithms.

1.2 ê¸°ì—¬
ì´ ë°±ì„œì—ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ì—°êµ¬ ê³µí—Œì´ ìˆìŠµë‹ˆë‹¤.

1. í•­ëª© ê¸°ë°˜ ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ì˜ ë¶„ì„ ë° í•˜ìœ„ ì‘ì—…ì„ êµ¬í˜„í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²• ì‹ë³„.

2. í•­ëª© ê¸°ë°˜ ê¶Œì¥ ì‚¬í•­ì˜ ì˜¨ë¼ì¸ í™•ì¥ ì„±ì„ ë†’ì´ê¸° ìœ„í•´ í•­ëª© ìœ ì‚¬ì„±ì˜ ì‚¬ì „ ê³„ì‚° ëœ ëª¨ë¸ì„ ê³µì‹í™”í•©ë‹ˆë‹¤.

3. ê¸°ì¡´ì˜ ì‚¬ìš©ì ê¸°ë°˜ (ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ) ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì—¬ëŸ¬ ê°€ì§€ í•­ëª© ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ í’ˆì§ˆì„ ì‹¤í—˜ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.








1.3 Organization
The rest of the paper is organized as follows. 

The next section provides a brief background in collaborative filtering algorithms. 

We first formally describe the collaborative filtering process and then discuss its two variants memorybased and model-based approaches. 

We then present some challenges associated with the memory-based approach. 

In section 3, we present the item-based approach and describe different sub-tasks of the algorithm in detail. 

Section 4 describes our experimental work. 

It provides details of our data sets, evaluation metrics, methodology and results of different experiments and discussion of the results. 

The final section provides some concluding remarks and directions for future research.

1.3 ì¡°ì§
ë‚˜ë¨¸ì§€ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤.

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ê°„ëµí•œ ë°°ê²½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ë¨¼ì € í˜‘ì—… í•„í„°ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ê³µì‹ì ìœ¼ë¡œ ì„¤ëª…í•œ ë‹¤ìŒ ë‘ ê°€ì§€ ë³€í˜• ë©”ëª¨ë¦¬ ê¸°ë°˜ ë° ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•´ ë…¼ì˜í•©ë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ê³¼ ê´€ë ¨ëœ ëª‡ ê°€ì§€ ë¬¸ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

ì„¹ì…˜ 3ì—ì„œëŠ” í•­ëª© ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•˜ê³  ì•Œê³ ë¦¬ì¦˜ì˜ ì—¬ëŸ¬ í•˜ìœ„ ì‘ì—…ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

ì„¹ì…˜ 4ëŠ” ìš°ë¦¬ì˜ ì‹¤í—˜ ì‘ì—…ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ë°ì´í„° ì„¸íŠ¸, í‰ê°€ ì§€í‘œ, ë°©ë²•ë¡  ë° ë‹¤ì–‘í•œ ì‹¤í—˜ì˜ ê²°ê³¼ ë° ê²°ê³¼ì— ëŒ€í•œ í† ë¡ ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ë§ˆì§€ë§‰ ì„¹ì…˜ì—ì„œëŠ” í–¥í›„ ì—°êµ¬ë¥¼ìœ„í•œ ëª‡ ê°€ì§€ ê²°ë¡  ë° ë°©í–¥ì„ ì œê³µí•©ë‹ˆë‹¤. 





2. COLLABORATIVE FILTERING BASED RECOMMENDER SYSTEMS

Recommender systems systems apply data analysis techniques to the problem of helping users find the items they would like to purchase at E-Commerce sites by producing a predicted likeliness score or a list of top{N recommended items for a given user. 

Item recommendations can be made using different methods. 

Recommendations can be based on demographics of the users, overall top selling items, or past buying habit of users as a predictor of future items.

Collaborative Filtering (CF) [19, 27] is the most successful recommendation technique to date. 

The basic idea of CF-based algorithms is to provide item recommendations or predictions based on the opinions of other like-minded users. 

The opinions of users can be obtained explicitly from the users or by using some implicit measures.

2.0.1 Overview of the Collaborative Filtering Process

The goal of a collaborative filtering algorithm is to suggest new items or to predict the utility of a certain item for a particular user based on the user's previous likings and the opinions of other like-minded users. 

In a typical CF scenario, there is a list of m users U = {u1, u2 ;::: ;um} and a list of n items I = {i1; i2 ;::: ;in}. 

Each user ui has a list of items Iui , which the user has expressed his/her opinions about. 

Opinions can be explicitly given by the user as a rating score, generally within a certain numerical scale, or can be implicitly derived from purchase records, by analyzing timing logs, by mining web hyperlinks and so on [28, 16].

Note that Iuiê³µì‹I and it is possible for Iui, to be a null-set. 

2. í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ

Recommender ì‹œìŠ¤í…œ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ì „ì ìƒê±°ë˜ ì‚¬ì´íŠ¸ì—ì„œ êµ¬ë§¤í•˜ê³  ì‹¶ì€ í•­ëª©ì„ ì°¾ë„ë¡ ë•ëŠ” ë¬¸ì œì— ë°ì´í„° ë¶„ì„ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•´ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì ìˆ˜ ë˜ëŠ” ìƒìœ„ {N ê°œ ê¶Œì¥ í•­ëª© ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.

í•­ëª© ì¶”ì²œì€ ë‹¤ë¥¸ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê¶Œì¥ ì‚¬í•­ì€ ì‚¬ìš©ìì˜ ì¸êµ¬ í†µê³„, ì „ì²´ì ìœ¼ë¡œ ê°€ì¥ ë§ì´ íŒ”ë¦° í•­ëª© ë˜ëŠ” ë¯¸ë˜ í•­ëª©ì˜ ì˜ˆì¸¡ ë³€ìˆ˜ë¡œì„œ ì‚¬ìš©ìì˜ ê³¼ê±° êµ¬ë§¤ ìŠµê´€ì„ ê¸°ë°˜ìœ¼ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

CF (Collaborative Filtering) [19, 27]ëŠ” í˜„ì¬ê¹Œì§€ ê°€ì¥ ì„±ê³µì ì¸ ì¶”ì²œ ê¸°ë²•ì…ë‹ˆë‹¤.

CF ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ê°™ì€ ìƒê°ì„ ê°€ì§„ ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ì˜ê²¬ì„ ê¸°ë°˜ìœ¼ë¡œ í•­ëª© ì¶”ì²œ ë˜ëŠ” ì˜ˆì¸¡ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì˜ê²¬ì€ ì‚¬ìš©ìë¡œë¶€í„° ëª…ì‹œ ì ìœ¼ë¡œ ë˜ëŠ” ì¼ë¶€ ì•”ì‹œ ì  ì¡°ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2.0.1 í˜‘ì—… í•„í„°ë§ í”„ë¡œì„¸ìŠ¤ ê°œìš”

í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì˜ ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ì´ì „ ì„ í˜¸ë„ì™€ ê°™ì€ ìƒê°ì„ ê°€ì§„ ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ì˜ê²¬ì„ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ì‚¬ìš©ìì—ê²Œ ìƒˆë¡œìš´ í•­ëª©ì„ ì œì•ˆí•˜ê±°ë‚˜ íŠ¹ì • í•­ëª©ì˜ ìœ ìš©ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì¼ë°˜ì ì¸ CF ì‹œë‚˜ë¦¬ì˜¤ì—ëŠ” m ëª…ì˜ ì‚¬ìš©ì ëª©ë¡ U = {u1, u2; :::; um} ë° n ê°œì˜ í•­ëª© ëª©ë¡ I = {i1; i2; :::; in}.

ê° ì‚¬ìš©ì uiì—ëŠ” ì‚¬ìš©ìê°€ ìì‹ ì˜ ì˜ê²¬ì„ í‘œí˜„í•œ Iui í•­ëª© ëª©ë¡ì´ ìˆìŠµë‹ˆë‹¤.

ì˜ê²¬ì€ ì¼ë°˜ì ìœ¼ë¡œ íŠ¹ì • ìˆ˜ì¹˜ ì²™ë„ ë‚´ì—ì„œ í‰ì  ì ìˆ˜ë¡œ ì‚¬ìš©ìì— ì˜í•´ ëª…ì‹œ ì ìœ¼ë¡œ ì œê³µë˜ê±°ë‚˜, íƒ€ì´ë° ë¡œê·¸ ë¶„ì„, ì›¹ í•˜ì´í¼ ë§í¬ ë§ˆì´ë‹ ë“±ì„ í†µí•´ êµ¬ë§¤ ê¸°ë¡ì—ì„œ ì•”ì‹œ ì ìœ¼ë¡œ ë„ì¶œ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ [28, 16].

Iui ê³µì‹ I ë° Iuiê°€ null ì§‘í•©ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.




There exists a distinguished user ua 2 U called the active user for whom the task of a collaborative filtering algorithm is to find an item likeliness that can be of two forms. 

* Prediction is a numerical value, Pa;j , expressing the predicted likeliness of item ij 62 Iua for the active user ua. 

This predicted value is within the same scale (e.g., from 1 to 5) as the opinion values provided by ua. 

* Recommendation is a list of N items, Ir  I, that the active user will like the most. 

Note that the recommended list must be on items not already purchased by the active user, i.e., Irì§‘í•©Iua = . 

This interface of CF algorithms is also known as Top-N recommendation. 

Figure 1 shows the schematic diagram of the collaborative filtering process. 

CF algorithms represent the entire m x n user-item data as a ratings matrix, A. 

Each entry ai;j in A represents the preference score (ratings) of the ith user on the jth item. 

Each individual ratings is within a numerical scale and it can as well be 0 indicating that the user has not yet rated that item. 

Researchers have devised a number of collaborative filtering algorithms that can be divided into two main categories|Memory-based (user-based) and Model-based (item-based) algorithms [6]. 

In this section we provide a detailed analysis of CF-based recommender system algorithms.

Memory-based Collaborative Filtering Algorithms. 

Memory-based algorithms utilize the entire user-item database to generate a prediction. 

ê³µë™ í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ì—…ì´ ë‘ ê°€ì§€ í˜•íƒœê°€ ë  ìˆ˜ìˆëŠ” í•­ëª© ìœ ì‚¬ì„±ì„ ì°¾ëŠ” ê²ƒ ì¸ í™œì„± ì‚¬ìš©ìë¼ê³ í•˜ëŠ” ê³ ìœ  ì‚¬ìš©ì ua 2 Uê°€ ìˆìŠµë‹ˆë‹¤.

* ì˜ˆì¸¡ì€ í™œì„± ì‚¬ìš©ì uaì— ëŒ€í•œ í•­ëª© ij 62 Iuaì˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì ê°’ Pa; jì…ë‹ˆë‹¤.

ì´ ì˜ˆì¸¡ ê°’ì€ uaì—ì„œ ì œê³µí•˜ëŠ” ì˜ê²¬ ê°’ê³¼ ë™ì¼í•œ ì²™ë„ (ì˜ˆ : 1 ~ 5) ë‚´ì— ìˆìŠµë‹ˆë‹¤.

* ì¶”ì²œ í•­ëª©ì€ í™œì„± ì‚¬ìš©ìê°€ ê°€ì¥ ì¢‹ì•„í•  N ê°œ í•­ëª© Ir Iì˜ ëª©ë¡ì…ë‹ˆë‹¤.

ê¶Œì¥ ëª©ë¡ì€ í™œì„± ì‚¬ìš©ìê°€ ì•„ì§ êµ¬ë§¤í•˜ì§€ ì•Šì€ í•­ëª© (ì˜ˆ : Ir ì§‘í•© Iua =)ì— ìˆì–´ì•¼í•©ë‹ˆë‹¤.

ì´ CF ì•Œê³ ë¦¬ì¦˜ ì¸í„°í˜ì´ìŠ¤ëŠ” Top-N ê¶Œì¥ ì‚¬í•­ì´ë¼ê³ ë„í•©ë‹ˆë‹¤.

ê·¸ë¦¼ 1ì€ í˜‘ì—… í•„í„°ë§ í”„ë¡œì„¸ìŠ¤ì˜ ê°œëµë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

CF ì•Œê³ ë¦¬ì¦˜ì€ ì „ì²´ m x n ì‚¬ìš©ì í•­ëª© ë°ì´í„°ë¥¼ ë“±ê¸‰ í–‰ë ¬ Aë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

Aì˜ ê° í•­ëª© ai; jëŠ” j ë²ˆì§¸ í•­ëª©ì— ëŒ€í•œ i ë²ˆì§¸ ì‚¬ìš©ìì˜ ì„ í˜¸ë„ ì ìˆ˜ (ë“±ê¸‰)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ê° ê°œë³„ ë“±ê¸‰ì€ ìˆ«ì ì²™ë„ ë‚´ì— ìˆìœ¼ë©° ì‚¬ìš©ìê°€ ì•„ì§ í•´ë‹¹ í•­ëª©ì„ í‰ê°€í•˜ì§€ ì•Šì•˜ ìŒì„ ë‚˜íƒ€ë‚´ëŠ” 0 ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ì—°êµ¬ìë“¤ì€ ë©”ëª¨ë¦¬ ê¸°ë°˜ (ì‚¬ìš©ì ê¸°ë°˜) ë° ëª¨ë¸ ê¸°ë°˜ (í•­ëª© ê¸°ë°˜) ì•Œê³ ë¦¬ì¦˜ [6]ì˜ ë‘ ê°€ì§€ ì£¼ìš” ë²”ì£¼ë¡œ ë‚˜ëˆŒ ìˆ˜ìˆëŠ” ì—¬ëŸ¬ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ê³ ì•ˆí–ˆìŠµë‹ˆë‹¤.

ì´ ì„¹ì…˜ì—ì„œëŠ” CF ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ìì„¸í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

ë©”ëª¨ë¦¬ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜.

ë©”ëª¨ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ì „ì²´ ì‚¬ìš©ì í•­ëª© ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.







These systems employ statistical techniques to find a set of users, known as neighbors, that have a history of agreeing with the target user (i.e., they either rate different items similarly or they tend to buy similar set of items). 

Once a neighborhood of users is formed, these systems use different algorithms to combine the preferences of neighbors to produce a prediction or top-N recommendation for the active user. 

The techniques, also known as nearest-neighbor or user-based collaborative filtering, are more popular and widely used in practice.

Model-based Collaborative Filtering Algorithms. 

Model-based collaborative filtering algorithms provide item recommendation by first developing a model of user ratings. 

Algorithms in this category take a probabilistic approach and envision the collaborative filtering process as computing the expected value of a user prediction, given his/her ratings on other items. 

The model building process is performed by different machine learning algorithms such as Bayesian network, clustering, and rule-based approaches. 

The Bayesian network model [6] formulates a probabilistic model for collaborative filtering problem. 

Clustering model treats collaborative filtering as a classification problem [2, 6, 29] and works by clustering similar users in same class and estimating the probability that a particular user is in a particular class C, and from there computes the conditional probability of ratings. 

The rule-based approach applies association rule discovery algorithms to find association between co-purchased items and then generates item recommendation based on the strength of the association between items[25].

2.0.2 Challenges of User-based Collaborative Filtering Algorithms
User-based collaborative filtering systems have been very successful in past, but their widespread use has revealed some potential challenges such as: 

ì´ëŸ¬í•œ ì‹œìŠ¤í…œì€ í†µê³„ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ìƒ ì‚¬ìš©ìì™€ ë™ì˜ í•œ ê¸°ë¡ì´ìˆëŠ” ì´ì›ƒì´ë¼ê³  ì•Œë ¤ì§„ ì‚¬ìš©ì ì§‘í•©ì„ ì°¾ìŠµë‹ˆë‹¤ (ì¦‰, ì„œë¡œ ë‹¤ë¥¸ í•­ëª©ì„ ë¹„ìŠ·í•˜ê²Œ í‰ê°€í•˜ê±°ë‚˜ ë¹„ìŠ·í•œ í•­ëª© ì§‘í•©ì„ êµ¬ì…í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ).

ì‚¬ìš©ìì˜ ì´ì›ƒì´ í˜•ì„±ë˜ë©´ ì´ëŸ¬í•œ ì‹œìŠ¤í…œì€ ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì´ì›ƒì˜ ì„ í˜¸ë„ë¥¼ ê²°í•©í•˜ì—¬ í™œì„± ì‚¬ìš©ìì— ëŒ€í•œ ì˜ˆì¸¡ ë˜ëŠ” top-N ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.

ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ë˜ëŠ” ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ì´ë¼ê³ ë„í•˜ëŠ”ì´ ê¸°ìˆ ì€ ë” ë„ë¦¬ ì‚¬ìš©ë˜ê³  ì‹¤ì œë¡œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.

ëª¨ë¸ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜.

ëª¨ë¸ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì€ ë¨¼ì € ì‚¬ìš©ì í‰ê°€ ëª¨ë¸ì„ ê°œë°œí•˜ì—¬ í•­ëª© ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.

ì´ ë²”ì£¼ì˜ ì•Œê³ ë¦¬ì¦˜ì€ í™•ë¥  ì  ì ‘ê·¼ ë°©ì‹ì„ ì·¨í•˜ê³  ë‹¤ë¥¸ í•­ëª©ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©ì ì˜ˆì¸¡ì˜ ì˜ˆìƒ ê°’ì„ ê³„ì‚°í•˜ëŠ” í˜‘ì—… í•„í„°ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ìƒí•©ë‹ˆë‹¤.

ëª¨ë¸ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤ëŠ” ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬, í´ëŸ¬ìŠ¤í„°ë§ ë° ê·œì¹™ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ ìˆ˜í–‰ë©ë‹ˆë‹¤.

ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ [6]ì€ í˜‘ì—… í•„í„°ë§ ë¬¸ì œì— ëŒ€í•œ í™•ë¥  ëª¨ë¸ì„ ê³µì‹í™”í•©ë‹ˆë‹¤.

í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ì€ í˜‘ì—… í•„í„°ë§ì„ ë¶„ë¥˜ ë¬¸ì œë¡œ ì·¨ê¸‰í•˜ê³  [2, 6, 29] ë™ì¼í•œ í´ë˜ìŠ¤ì˜ ìœ ì‚¬í•œ ì‚¬ìš©ìë¥¼ í´ëŸ¬ìŠ¤í„°ë§í•˜ê³  íŠ¹ì • ì‚¬ìš©ìê°€ íŠ¹ì • í´ë˜ìŠ¤ Cì— ì†í•  í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•˜ë©° ì—¬ê¸°ì—ì„œ ë“±ê¸‰ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

ê·œì¹™ ê¸°ë°˜ ì ‘ê·¼ë²•ì€ ì—°ê´€ ê·œì¹™ ë°œê²¬ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ê³µë™ êµ¬ë§¤ í’ˆëª© ê°„ì˜ ì—°ê´€ì„±ì„ ì°¾ì€ ë‹¤ìŒ í’ˆëª© ê°„ì˜ ì—°ê´€ ê°•ë„ì— ë”°ë¼ í’ˆëª© ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤ [25].

2.0.2 ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì˜ ê³¼ì œ
ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì‹œìŠ¤í…œì€ ê³¼ê±°ì— ë§¤ìš° ì„±ê³µì  ì´ì—ˆì§€ë§Œ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©ë˜ë©´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ëª‡ ê°€ì§€ ì ì¬ì  ì¸ ë¬¸ì œê°€ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.





* Sparsity. 

In practice, many commercial recommender systems are used to evaluate large item sets (e.g., Amazon.com recommends books and CDnow.com recommends music albums). 

In these systems, even active users may have purchased well under 1% of the items (1% of 2 million books is 20; 000 books). 

Accordingly, a recommender system based on nearest neighbor algorithms may be unable to make any item recommendations for a particular user. 

As a result the accuracy of recommendations may be poor. 

* Scalability. 

Nearest neighbor algorithms require computation that grows with both the number of users and the number of items. 

With millions of users and items, a typical web-based recommender system running existing algorithms will suffer serious scalability problems.

The weakness of nearest neighbor algorithm for large, sparse databases led us to explore alternative recommender system algorithms. 

Our first approach attempted to bridge the sparsity by incorporating semi-intelligent filtering agents into the system [23, 11]. 

These agents evaluated and rated each item using syntactic features. 

By providing a dense ratings set, they helped alleviate coverage and improved quality. 

The filtering agent solution, however, did not address the fundamental problem of poor relationships among like-minded but sparse-rating users. 


* í™•ì¥ ì„±.

ìµœê·¼ ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ì‚¬ìš©ì ìˆ˜ì™€ í•­ëª© ìˆ˜ì— ë”°ë¼ ì¦ê°€í•˜ëŠ” ê³„ì‚°ì´ í•„ìš”í•©ë‹ˆë‹¤.

ìˆ˜ë°±ë§Œ ëª…ì˜ ì‚¬ìš©ìì™€ í•­ëª©ì´ìˆëŠ” ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤í–‰í•˜ëŠ” ì¼ë°˜ì ì¸ ì›¹ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì€ ì‹¬ê°í•œ í™•ì¥ ì„± ë¬¸ì œë¥¼ ê²ªê²Œë©ë‹ˆë‹¤.

ëŒ€ê·œëª¨ í¬ì†Œ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ ìµœê·¼ ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì˜ ì•½ì ìœ¼ë¡œ ì¸í•´ ëŒ€ì²´ ì¶”ì²œ ì‹œìŠ¤í…œ ì•Œê³ ë¦¬ì¦˜ì„ íƒìƒ‰í•˜ê²Œë˜ì—ˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ì˜ ì²« ë²ˆì§¸ ì ‘ê·¼ ë°©ì‹ì€ ë°˜ ì§€ëŠ¥ì ì¸ í•„í„°ë§ ì—ì´ì „íŠ¸ë¥¼ ì‹œìŠ¤í…œì— í†µí•©í•˜ì—¬ í¬ì†Œì„±ì„ ì—°ê²°í•˜ë ¤ê³  ì‹œë„í–ˆìŠµë‹ˆë‹¤ [23, 11].

ì´ëŸ¬í•œ ì—ì´ì „íŠ¸ëŠ” êµ¬ë¬¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ê° í•­ëª©ì„ í‰ê°€í•˜ê³  í‰ê°€í–ˆìŠµë‹ˆë‹¤.

ë°€ë„ê°€ ë†’ì€ ë“±ê¸‰ ì„¸íŠ¸ë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ ì ìš© ë²”ìœ„ë¥¼ ì¤„ì´ê³  í’ˆì§ˆì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ë˜ì—ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ í•„í„°ë§ ì—ì´ì „íŠ¸ ì†”ë£¨ì…˜ì€ ìƒê°ì´ ë¹„ìŠ·í•˜ì§€ë§Œ ë“œë¬¸ ì‚¬ìš©ì ê°„ì˜ ê´€ê³„ ë¶ˆëŸ‰ì´ë¼ëŠ” ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.






To explore that we took an algorithmic approach and used Latent Semantic Indexing (LSI) to capture the similarity between users and items in a reduced dimensional space [24, 25]. 

In this paper we look into another technique, the model-based approach, in addressing these challenges, especially the scalability challenge. 

The main idea here is to analyze the user-item representation matrix to identify relations between different items and then to use these relations to compute the prediction score for a given user-item pair. 

The intuition behind this approach is that a user would be interested in purchasing items that are similar to the items the user liked earlier and would tend to avoid items that are similar to the items the user didn't like earlier. 

These techniques don't require to identify the neighborhood of similar users when a recommendation is requested; as a result they tend to produce much faster recommendations. 

A number of different schemes have been proposed to compute the association between items ranging from probabilistic approach [6] to more traditional item-item correlations [15, 13]. 

We present a detailed analysis of our approach in the next section.

ìš°ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ ì ‘ê·¼ ë°©ì‹ì„ ì·¨í•˜ê³  LSI (Latent Semantic Indexing)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶•ì†Œ ëœ ì°¨ì› ê³µê°„ì—ì„œ ì‚¬ìš©ìì™€ í•­ëª© ê°„ì˜ ìœ ì‚¬ì„±ì„ í¬ì°©í–ˆìŠµë‹ˆë‹¤ [24, 25].

ì´ ë°±ì„œì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œ, íŠ¹íˆ í™•ì¥ ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ìœ„í•œ ë˜ ë‹¤ë¥¸ ê¸°ìˆ ì¸ ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚´í´ ë´…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ì£¼ìš” ì•„ì´ë””ì–´ëŠ” ì‚¬ìš©ì í•­ëª© í‘œí˜„ í–‰ë ¬ì„ ë¶„ì„í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ í•­ëª© ê°„ì˜ ê´€ê³„ë¥¼ ì‹ë³„ í•œ ë‹¤ìŒ ì´ëŸ¬í•œ ê´€ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì‚¬ìš©ì í•­ëª© ìŒì— ëŒ€í•œ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ ì ‘ê·¼ ë°©ì‹ì˜ ì§ê´€ì€ ì‚¬ìš©ìê°€ ì´ì „ì— ì¢‹ì•„í–ˆë˜ í•­ëª©ê³¼ ìœ ì‚¬í•œ í•­ëª©ì„ êµ¬ë§¤í•˜ëŠ” ë° ê´€ì‹¬ì´ ìˆê³  ì‚¬ìš©ìê°€ ì´ì „ì— ì¢‹ì•„í•˜ì§€ ì•Šì•˜ë˜ í•­ëª©ê³¼ ìœ ì‚¬í•œ í•­ëª©ì„ í”¼í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì¶”ì²œì´ ìš”ì²­ ë  ë•Œ ìœ ì‚¬í•œ ì‚¬ìš©ìì˜ ì´ì›ƒì„ ì‹ë³„ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ í›¨ì”¬ ë” ë¹ ë¥¸ ê¶Œì¥ ì‚¬í•­ì„ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

í™•ë¥  ë¡ ì  ì ‘ê·¼ [6]ì—ì„œë³´ë‹¤ ì „í†µì ì¸ í•­ëª©-í•­ëª© ìƒê´€ [15, 13]ì— ì´ë¥´ëŠ” í•­ëª© ê°„ì˜ ì—°ê´€ì„±ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°€ì§€ ë‹¤ë¥¸ ë°©ì‹ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ ìì„¸í•œ ë¶„ì„ì„ ì œì‹œí•©ë‹ˆë‹¤.





3. ITEM-BASED COLLABORATIVE FILTERING ALGORITHM
In this section we study a class of item-based recommendation algorithms for producing predictions to users. 

Unlike the user-based collaborative filtering algorithm discussed in Section 2, the item-based approach looks into the set of items the target user has rated and computes how similar they are to the target item i and then selects k most similar items fi1; i2;::: ;ikg. 

At the same time their corresponding similarities {si1; si2;::: ;sik} are also computed.

Once the most similar items are found, the prediction is then computed by taking a weighted average of the target user's ratings on these similar items. 

We describe these two aspects, namely, the similarity computation and the prediction generation in details here. 

3. í•­ëª© ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜
ì´ ì„¹ì…˜ì—ì„œëŠ” ì‚¬ìš©ìì—ê²Œ ì˜ˆì¸¡ì„ ìƒì„±í•˜ê¸°ìœ„í•œ í•­ëª© ê¸°ë°˜ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤.

ì„¹ì…˜ 2ì—ì„œ ë…¼ì˜ ëœ ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ê³¼ ë‹¬ë¦¬, í•­ëª© ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì€ ëŒ€ìƒ ì‚¬ìš©ìê°€ í‰ê°€ í•œ í•­ëª© ì„¸íŠ¸ë¥¼ ì¡°ì‚¬í•˜ê³  ëŒ€ìƒ í•­ëª© iì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œ ì§€ ê³„ì‚° í•œ ë‹¤ìŒ k ê°œì˜ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª© fi1ì„ ì„ íƒí•©ë‹ˆë‹¤. i2; :::; ikg.

ë™ì‹œì— ê·¸ë“¤ì˜ ìœ ì‚¬ì„± {si1; si2; :::; sik}ë„ ê³„ì‚°ë©ë‹ˆë‹¤.

ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©ì´ ë°œê²¬ë˜ë©´ ì´ëŸ¬í•œ ìœ ì‚¬í•œ í•­ëª©ì— ëŒ€í•œ ëŒ€ìƒ ì‚¬ìš©ì í‰ê°€ì˜ ê°€ì¤‘ í‰ê· ì„ ì·¨í•˜ì—¬ ì˜ˆì¸¡ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

ì´ ë‘ ê°€ì§€ ì¸¡ë©´, ì¦‰ ìœ ì‚¬ì„± ê³„ì‚°ê³¼ ì˜ˆì¸¡ ìƒì„±ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.




3.1 Item Similarity Computation 

One critical step in the item-based collaborative filtering algorithm is to compute the similarity between items and then to select the most similar items. 

The basic idea in similarity computation between two items i and j is to first isolate the users who have rated both of these items and then to apply a similarity computation technique to determine the similarity si;j . 

Figure 2 illustrates this process; here the matrix rows represent users and the columns represent items.

There are a number of different ways to compute the similarity between items. 

Here we present three such methods.

These are cosine-based similarity, correlation-based similarity and adjusted-cosine similarity.

3.1 í•­ëª© ìœ ì‚¬ì„± ê³„ì‚°

í•­ëª© ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì˜ ì¤‘ìš”í•œ ë‹¨ê³„ ì¤‘ í•˜ë‚˜ëŠ” í•­ëª© ê°„ì˜ ìœ ì‚¬ì„±ì„ ê³„ì‚° í•œ ë‹¤ìŒ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©ì„ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë‘ í•­ëª© i ë° j ê°„ì˜ ìœ ì‚¬ì„± ê³„ì‚°ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë¨¼ì € ì´ëŸ¬í•œ í•­ëª©ì„ ëª¨ë‘ í‰ê°€ í•œ ì‚¬ìš©ìë¥¼ ë¶„ë¦¬ í•œ ë‹¤ìŒ ìœ ì‚¬ì„± ê³„ì‚° ê¸°ìˆ ì„ ì ìš©í•˜ì—¬ ìœ ì‚¬ì„± si; jë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ê·¸ë¦¼ 2ëŠ”ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì—¬ê¸°ì„œ í–‰ë ¬ í–‰ì€ ì‚¬ìš©ìë¥¼ ë‚˜íƒ€ë‚´ê³  ì—´ì€ í•­ëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

í•­ëª© ê°„ì˜ ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ê·¸ëŸ¬í•œ ì„¸ ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

ì´ë“¤ì€ ì½”ì‚¬ì¸ ê¸°ë°˜ ìœ ì‚¬ì„±, ìƒê´€ ê¸°ë°˜ ìœ ì‚¬ì„± ë° ì¡°ì • ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì…ë‹ˆë‹¤.





3.1.1 Cosine-based Similarity
In this case, two items are thought of as two vectors in the m dimensional user-space. 

The similarity between them is measured by computing the cosine of the angle between these two vectors. 

Formally, in the m  n ratings matrix in Figure 2, similarity between items i and j, denoted by
sim(i; j) is given by
sim(i; j)ê³µì‹
where "$ \cdot $" denotes the dot-product of the two vectors.

3.1.2 Correlation-based Similarity
In this case, similarity between two items i and j is measured by computing the Pearson-r correlation corri;j . 

To make the correlation computation accurate we must first isolate the co-rated cases (i.e., cases where the users rated both i and j) as shown in Figure 2. 

Let the set of users who both rated i and j are denoted by U then the correlation similarity is given by
sim(i; j) ê³µì‹
Here R_ui denotes the rating of user u on item i, Ri is the average rating of the i-th item.


3.1.1 ì½”ì‚¬ì¸ ê¸°ë°˜ ìœ ì‚¬ì„±
ì´ ê²½ìš° ë‘ í•­ëª©ì€ m ì°¨ì› ì‚¬ìš©ì ê³µê°„ì—ì„œ ë‘ ë²¡í„°ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.

ì´ë“¤ ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì€ì´ ë‘ ë²¡í„° ì‚¬ì´ ê°ë„ì˜ ì½”ì‚¬ì¸ì„ ê³„ì‚°í•˜ì—¬ ì¸¡ì •ë©ë‹ˆë‹¤.

ê³µì‹ì ìœ¼ë¡œ ê·¸ë¦¼ 2ì˜ m n ë“±ê¸‰ í–‰ë ¬ì—ì„œ í•­ëª© iì™€ j ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œë©ë‹ˆë‹¤.
sim (i; j)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§„ë‹¤.
sim (i; j) ê³µì‹
ì—¬ê¸°ì„œ "$ \ cdot $"ëŠ” ë‘ ë²¡í„°ì˜ ë‚´ì ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

3.1.2 ìƒê´€ ê¸°ë°˜ ìœ ì‚¬ì„±
ì´ ê²½ìš° ë‘ í•­ëª© iì™€ j ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì€ Pearson-r ìƒê´€ corri; jë¥¼ ê³„ì‚°í•˜ì—¬ ì¸¡ì •ë©ë‹ˆë‹¤.

ìƒê´€ ê´€ê³„ ê³„ì‚°ì„ ì •í™•í•˜ê²Œí•˜ê¸° ìœ„í•´ ë¨¼ì € ê·¸ë¦¼ 2ì™€ ê°™ì´ ê³µë™ í‰ê°€ ëœ ì¼€ì´ìŠ¤ (ì¦‰, ì‚¬ìš©ìê°€ iì™€ jë¥¼ ëª¨ë‘ í‰ê°€ í•œ ì¼€ì´ìŠ¤)ë¥¼ ë¶„ë¦¬í•´ì•¼í•©ë‹ˆë‹¤.

iì™€ jë¥¼ ëª¨ë‘ í‰ê°€ í•œ ì‚¬ìš©ì ì„¸íŠ¸ë¥¼ Uë¡œ í‘œì‹œí•˜ë©´ ìƒê´€ ìœ ì‚¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œë©ë‹ˆë‹¤.
sim (i; j) ê³µì‹
ì—¬ê¸°ì„œ R_uiëŠ” í•­ëª© iì— ëŒ€í•œ ì‚¬ìš©ì uì˜ ë“±ê¸‰ì„ ë‚˜íƒ€ë‚´ê³  RiëŠ” i ë²ˆì§¸ í•­ëª©ì˜ í‰ê·  ë“±ê¸‰ì…ë‹ˆë‹¤.







3.1.3 Adjusted Cosine Similarity
One fundamental difference between the similarity computation in user-based CF and item-based CF is that in case of user-based CF the similarity is computed along the rows of the matrix but in case of the item-based CF the similarity is computed along the columns, i.e., each pair in the co-rated set corresponds to a different user (Figure 2). 

Computing similarity using basic cosine measure in item-based case has one important drawback|the differences in rating scale between different users are not taken into account.

The adjusted cosine similarity offsets this drawback by subtracting the corresponding user average from each co-rated pair. 

Formally, the similarity between items i and j using this scheme is given by
sim(i; j)ê³µì‹ 
Here Ru is the average of the u-th user's ratings.

3.1.3 ì¡°ì • ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±
ì‚¬ìš©ì ê¸°ë°˜ CFì™€ í•­ëª© ê¸°ë°˜ CFì˜ ìœ ì‚¬ì„± ê³„ì‚°ì˜ ê·¼ë³¸ì ì¸ ì°¨ì´ì  ì¤‘ í•˜ë‚˜ëŠ” ì‚¬ìš©ì ê¸°ë°˜ CFì˜ ê²½ìš° ìœ ì‚¬ì„±ì´ í–‰ë ¬ì˜ í–‰ì„ ë”°ë¼ ê³„ì‚°ë˜ì§€ë§Œ í•­ëª© ê¸°ë°˜ CFì˜ ê²½ìš° ìœ ì‚¬ì„±ì´ í•¨ê»˜ ê³„ì‚°ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰, ê³µë™ ë“±ê¸‰ ì„¸íŠ¸ì˜ ê° ìŒì€ ì„œë¡œ ë‹¤ë¥¸ ì‚¬ìš©ìì— í•´ë‹¹í•©ë‹ˆë‹¤ (ê·¸ë¦¼ 2).

í•­ëª© ê¸°ë°˜ ì‚¬ë¡€ì—ì„œ ê¸°ë³¸ ì½”ì‚¬ì¸ ì²™ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì€ í•œ ê°€ì§€ ì¤‘ìš”í•œ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì‚¬ìš©ì ê°„ì˜ í‰ê°€ ì²™ë„ ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

ì¡°ì • ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì€ ê° ê³µë™ ë“±ê¸‰ ìŒì—ì„œ í•´ë‹¹ ì‚¬ìš©ì í‰ê· ì„ ë¹¼ì„œì´ ë‹¨ì ì„ ìƒì‡„í•©ë‹ˆë‹¤.

ê³µì‹ì ìœ¼ë¡œ,ì´ ì²´ê³„ë¥¼ ì‚¬ìš©í•˜ëŠ” í•­ëª© iì™€ j ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§„ë‹¤.
sim (i; j) ê³µì‹
ì—¬ê¸°ì„œ RuëŠ” u ë²ˆì§¸ ì‚¬ìš©ìì˜ í‰ì  í‰ê· ì…ë‹ˆë‹¤.






3.2 Prediction Computation
The most important step in a collaborative filtering system is to generate the output interface in terms of prediction.

Once we isolate the set of most similar items based on the similarity measures, the next step is to look into the target users ratings and use a technique to obtain predictions. 

Here we consider two such techniques.

3.2.1 Weighted Sum
As the name implies, this method computes the prediction on an item i for a user u by computing the sum of the ratings given by the user on the items similar to i. 

Each ratings is weighted by the corresponding similarity si;j between items i and j. 

Formally, using the notion shown in Figure 3 we can denote the prediction Pu;i as 
Puiê³µì‹
Basically, this approach tries to capture how the active user rates the similar items. 

The weighted sum is scaled by the sum of the similarity terms to make sure the prediction is within the predefined range.

3.2 ì˜ˆì¸¡ ê³„ì‚°
í˜‘ì—… í•„í„°ë§ ì‹œìŠ¤í…œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„ëŠ” ì˜ˆì¸¡ ì¸¡ë©´ì—ì„œ ì¶œë ¥ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ìœ ì‚¬ì„± ì¸¡ì • ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª© ì§‘í•©ì„ ë¶„ë¦¬ í•œ í›„ ë‹¤ìŒ ë‹¨ê³„ëŠ” ëŒ€ìƒ ì‚¬ìš©ì ë“±ê¸‰ì„ ì¡°ì‚¬í•˜ê³  ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ì–»ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ê·¸ëŸ¬í•œ ë‘ ê°€ì§€ ê¸°ìˆ ì„ ê³ ë ¤í•©ë‹ˆë‹¤.

3.2.1 ê°€ì¤‘ í•©ê³„
ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ì´ ë°©ë²•ì€ iì™€ ìœ ì‚¬í•œ í•­ëª©ì— ëŒ€í•´ ì‚¬ìš©ìê°€ ë¶€ì—¬í•œ ë“±ê¸‰ì˜ í•©ê³„ë¥¼ ê³„ì‚°í•˜ì—¬ ì‚¬ìš©ì uì— ëŒ€í•œ í•­ëª© iì— ëŒ€í•œ ì˜ˆì¸¡ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

ê° ë“±ê¸‰ì€ í•­ëª© iì™€ j ì‚¬ì´ì˜ í•´ë‹¹ ìœ ì‚¬ì„± si; jì— ì˜í•´ ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.

ê³µì‹ì ìœ¼ë¡œ ê·¸ë¦¼ 3ì˜ ê°œë…ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ Pu; ië¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í‘¸ì´ ê³µì‹
ê¸°ë³¸ì ìœ¼ë¡œì´ ì ‘ê·¼ ë°©ì‹ì€ í™œì„± ì‚¬ìš©ìê°€ ìœ ì‚¬í•œ í•­ëª©ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ í¬ì°©í•˜ë ¤ê³ í•©ë‹ˆë‹¤.

ê°€ì¤‘ í•©ê³„ëŠ” ì˜ˆì¸¡ì´ ì‚¬ì „ ì •ì˜ ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ìœ ì‚¬ì„± í•­ì˜ í•©ê³„ë¡œ ì¡°ì •ë©ë‹ˆë‹¤.






3.2.2 Regression

This approach is similar to the weighted sum method but instead of directly using the ratings of similar items it uses an approximation of the ratings based on regression model.

In practice, the similarities computed using cosine or correlation measures may be misleading in the sense that two rating vectors may be distant (in Euclidean sense) yet may have very high similarity. 

In that case using the raw ratings of the "so-called" similar item may result in poor prediction.

The basic idea is to use the same formula as the weighted
sum technique, but instead of using the similar item N's "raw" ratings values RuN's, this model uses their approximated values RuN based on a linear regression model. 

If we denote the respective vectors of the target item i and the similar item N by Ri and RN the linear regression model can be expressed as 

RN ê³µì‹  

The regression model parameters ì•ŒíŒŒ and ë² íƒ€ are determined by going over both of the rating vectors. 

ì…ì‹¤ë¡  is the error of the regression model.

3.2.2 íšŒê·€

ì´ ì ‘ê·¼ ë°©ì‹ì€ ê°€ì¤‘ í•©ê³„ ë°©ë²•ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ìœ ì‚¬í•œ í•­ëª©ì˜ ë“±ê¸‰ì„ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  íšŒê·€ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë“±ê¸‰ì˜ ê·¼ì‚¬ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì‹¤ì œë¡œ ì½”ì‚¬ì¸ ë˜ëŠ” ìƒê´€ ì¸¡ì •ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚° ëœ ìœ ì‚¬ì„±ì€ ë‘ ë“±ê¸‰ ë²¡í„°ê°€ ë©€ë¦¬ ë–¨ì–´ì ¸ìˆì„ ìˆ˜ ìˆì§€ë§Œ (ìœ í´ë¦¬ë“œ ì˜ë¯¸ì—ì„œ) ë§¤ìš° ë†’ì€ ìœ ì‚¬ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆìŠµë‹ˆë‹¤.

ì´ ê²½ìš° "ì†Œìœ„"ìœ ì‚¬í•œ í•­ëª©ì˜ ì›ì‹œ ë“±ê¸‰ì„ ì‚¬ìš©í•˜ë©´ ì˜ˆì¸¡ì´ ì¢‹ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ê°€ì¤‘ì¹˜ ë¶€ì—¬ì™€ ë™ì¼í•œ ê³µì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ì´ ëª¨ë¸ì€ ìœ ì‚¬í•œ í•­ëª© Nì˜ "ì›ì‹œ"ë“±ê¸‰ ê°’ RuNì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ì„ í˜• íšŒê·€ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œí•˜ëŠ” ê·¼ì‚¬ê°’ RuNì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

Ri ë° RNìœ¼ë¡œ ëŒ€ìƒ í•­ëª© i ë° ìœ ì‚¬í•œ í•­ëª© Nì˜ ê° ë²¡í„°ë¥¼ í‘œì‹œí•˜ë©´ ì„ í˜• íšŒê·€ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

RN ê³µì‹

íšŒê·€ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ ì•ŒíŒŒ ë° ë² íƒ€ëŠ” ë‘ ë“±ê¸‰ ë²¡í„°ë¥¼ ëª¨ë‘ ê²€í† í•˜ì—¬ ê²°ì •ë©ë‹ˆë‹¤.

ì…ì‹¤ë¡ ì€ íšŒê·€ ëª¨í˜•ì˜ ì˜¤ì°¨ì…ë‹ˆë‹¤.






3.3 Performance Implications

The largest E-Commerce sites operate at a scale that stresses the direct implementation of collaborative filtering.

In neighborhood-based CF systems, the neighborhood formation process, especially the user-user similarity computation step turns out to be the performance bottleneck, which in turn can make the whole process unsuitable for real-time recommendation generation. 

One way of ensuring high scalability is to use a model-based approach. 

Model-based systems have the potential to contribute to recommender systems to operate at a high scale. 

The main idea here to isolate the neighborhood generation and prediction generation steps.

In this paper, we present a model-based approach to precompute item-item similarity scores. 

The similarity computation scheme is still correlation-based but the computation is performed on the item space. 

In a typical E-Commerce scenario, we usually have a set of item that is static compared to the number of users that changes most often. 

The static nature of items leads us to the idea of precomputing the item similarities. 

One possible way of precomputing the item similarities is to compute all-to-all similarity and then performing a quick table look-up to retrieve the required similarity values. 

This method, although saves time, requires an O(n^2) space for n items.

The fact that we only need a small fraction of similar items to compute predictions leads us to an alternate model-based scheme. 

In this scheme, we retain only a small number of similar items. 

For each item j we compute the k most similar items, where k  n and record these item numbers and their similarities with j. 

We term k as the model size. 

3.3 ì„±ëŠ¥ ì˜í–¥

ê°€ì¥ í° ì „ì ìƒê±°ë˜ ì‚¬ì´íŠ¸ëŠ” í˜‘ì—… í•„í„°ë§ì˜ ì§ì ‘ì ì¸ êµ¬í˜„ì„ ê°•ì¡°í•˜ëŠ” ê·œëª¨ë¡œ ìš´ì˜ë©ë‹ˆë‹¤.

ì´ì›ƒ ê¸°ë°˜ CF ì‹œìŠ¤í…œì—ì„œ ì´ì›ƒ í˜•ì„± í”„ë¡œì„¸ìŠ¤, íŠ¹íˆ ì‚¬ìš©ì-ì‚¬ìš©ì ìœ ì‚¬ì„± ê³„ì‚° ë‹¨ê³„ëŠ” ì„±ëŠ¥ ë³‘ëª© í˜„ìƒìœ¼ë¡œ ë°í˜€ì ¸ ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤ì‹œê°„ ì¶”ì²œ ìƒì„±ì— ì í•©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë†’ì€ í™•ì¥ ì„±ì„ ë³´ì¥í•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì€ ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ëª¨ë¸ ê¸°ë°˜ ì‹œìŠ¤í…œì€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ëŒ€ê·œëª¨ë¡œ ì‘ë™í•˜ëŠ” ë° ê¸°ì—¬í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œ ì£¼ìš” ì•„ì´ë””ì–´ëŠ” ì´ì›ƒ ìƒì„± ë° ì˜ˆì¸¡ ìƒì„± ë‹¨ê³„ë¥¼ ë¶„ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ ë°±ì„œì—ì„œëŠ” í•­ëª©-í•­ëª© ìœ ì‚¬ì„± ì ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ê¸°ìœ„í•œ ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤.

ìœ ì‚¬ì„± ê³„ì‚° ì²´ê³„ëŠ” ì—¬ì „íˆ ìƒê´€ ê´€ê³„ ê¸°ë°˜ì´ì§€ë§Œ ê³„ì‚°ì€ í•­ëª© ê³µê°„ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.

ì¼ë°˜ì ì¸ ì „ì ìƒê±°ë˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ìì£¼ ë³€ê²½ë˜ëŠ” ì‚¬ìš©ì ìˆ˜ì— ë¹„í•´ ì •ì  ì¸ í•­ëª© ì§‘í•©ì´ ìˆìŠµë‹ˆë‹¤.

í•­ëª©ì˜ ì •ì  íŠ¹ì„±ì€ í•­ëª© ìœ ì‚¬ì„±ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ëŠ” ì•„ì´ë””ì–´ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.

í•­ëª© ìœ ì‚¬ì„±ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ëŠ” í•œ ê°€ì§€ ê°€ëŠ¥í•œ ë°©ë²•ì€ ì „ì²´ ìœ ì‚¬ì„±ì„ ê³„ì‚° í•œ ë‹¤ìŒ ë¹ ë¥¸ í…Œì´ë¸” ì¡°íšŒë¥¼ ìˆ˜í–‰í•˜ì—¬ í•„ìš”í•œ ìœ ì‚¬ì„± ê°’ì„ ê²€ìƒ‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ ë°©ë²•ì€ ì‹œê°„ì„ ì ˆì•½í•˜ì§€ë§Œ n ê°œ í•­ëª©ì— ëŒ€í•´ O (n ^ 2) ê³µê°„ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì˜ˆì¸¡ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ìœ ì‚¬í•œ í•­ëª©ì˜ ì‘ì€ ë¶€ë¶„ ë§Œ í•„ìš”í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì€ ìš°ë¦¬ë¥¼ ëŒ€ì²´ ëª¨ë¸ ê¸°ë°˜ ì²´ê³„ë¡œì´ ë•ë‹ˆë‹¤.

ì´ ê³„íšì—ì„œëŠ” ì†Œìˆ˜ì˜ ìœ ì‚¬í•œ í•­ëª© ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.

ê° í•­ëª© jì— ëŒ€í•´ ìš°ë¦¬ëŠ” k n ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©ì„ ê³„ì‚°í•˜ê³  ì´ëŸ¬í•œ í•­ëª© ë²ˆí˜¸ì™€ ê·¸ ìœ ì‚¬ì„±ì„ jë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.

kë¥¼ ëª¨ë¸ í¬ê¸°ë¼ê³ í•©ë‹ˆë‹¤.







Based on this model building step, our prediction generation algorithm works as follows. 

For generating predictions for a user u on item i, our algorithm first retrieves the precomputed k most similar items corresponding to the target item i. 

Then it looks how many of those k items were purchased by the user u, based on this intersection then the prediction is computed using basic item-based collaborative filtering algorithm. 

We observe a quality-performance trade-off here: to ensure good quality we must have a large model size, which leads to the performance problems discussed above. 

In one extreme, we can have a model size of n, which will ensure the exact same quality as the original scheme but will have high space complexity. 

However, our model building step ensures that we retain the most similar items. 

While generating predictions, these items contribute the most to the prediction scores. 

Accordingly, we hypothesize that this model-based approach will provide reasonably good prediction quality with even a small model size and hence provide a good performance. 

We experimentally validate our hypothesis later in this paper. 

In particular, we experiment with the model size by varying the number of similar items to be stored.

Then we perform experiments to compute prediction and response-time to determine the impact of the model size on quality and performance of the whole system.

ì´ ëª¨ë¸ êµ¬ì¶• ë‹¨ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ìƒì„± ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤.

í•­ëª© iì— ëŒ€í•œ ì‚¬ìš©ì uì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì•Œê³ ë¦¬ì¦˜ì€ ë¨¼ì € ëŒ€ìƒ í•­ëª© iì— í•´ë‹¹í•˜ëŠ” ì‚¬ì „ ê³„ì‚° ëœ k ê°œì˜ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ ì‚¬ìš©ì uê°€ êµ¬ë§¤ í•œ k ê°œì˜ í•­ëª© ì¤‘ ëª‡ ê°œë¥¼ì´ êµì°¨ì ì„ ê¸°ë°˜ìœ¼ë¡œí•˜ì—¬ ì˜ˆì¸¡ì€ ê¸°ë³¸ í•­ëª© ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ í’ˆì§ˆ-ì„±ëŠ¥ ì ˆì¶©ì•ˆì„ ê´€ì°°í•©ë‹ˆë‹¤. ì¢‹ì€ í’ˆì§ˆì„ ë³´ì¥í•˜ë ¤ë©´ ëª¨ë¸ í¬ê¸°ê°€ ì»¤ì•¼í•˜ë©°, ì´ë¡œ ì¸í•´ ìœ„ì—ì„œ ì„¤ëª…í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

í•œ ê°€ì§€ ê·¹ë‹¨ì  ì¸ ê²½ìš°, ëª¨ë¸ í¬ê¸° nì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ì›ë˜ ê³„íšê³¼ ë˜‘ê°™ì€ í’ˆì§ˆì„ ë³´ì¥í•˜ì§€ë§Œ ê³µê°„ ë³µì¡ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ëª¨ë¸ êµ¬ì¶• ë‹¨ê³„ì—ì„œëŠ” ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©ì„ ìœ ì§€í•©ë‹ˆë‹¤.

ì˜ˆì¸¡ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì´ëŸ¬í•œ í•­ëª©ì€ ì˜ˆì¸¡ ì ìˆ˜ì— ê°€ì¥ ë§ì´ ê¸°ì—¬í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ ìš°ë¦¬ëŠ”ì´ ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì´ ì‘ì€ ëª¨ë¸ í¬ê¸°ë¡œë„ í•©ë¦¬ì ìœ¼ë¡œ ì¢‹ì€ ì˜ˆì¸¡ í’ˆì§ˆì„ ì œê³µí•˜ì—¬ ì¢‹ì€ ì„±ëŠ¥ì„ ì œê³µ í•  ê²ƒì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.

ìš°ë¦¬ëŠ”ì´ ë°±ì„œ ë’·ë¶€ë¶„ì—ì„œ ìš°ë¦¬ì˜ ê°€ì„¤ì„ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.

íŠ¹íˆ, ì €ì¥í•  ìœ ì‚¬í•œ í•­ëª©ì˜ ìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ì‹¤í—˜í•©ë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ ì˜ˆì¸¡ ë° ì‘ë‹µ ì‹œê°„ì„ ê³„ì‚°í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ í¬ê¸°ê°€ ì „ì²´ ì‹œìŠ¤í…œì˜ í’ˆì§ˆ ë° ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í™•ì¸í•©ë‹ˆë‹¤.






4. EXPERIMENTAL EVALUATION
4.1 Data set

We used experimental data from our research website to evaluate different variants of item-based recommendation algorithms.

Movie data. 

We used data from our MovieLens recommender system. 

MovieLens is a web-based research recommender system that debuted in Fall 1997. 

Each week hundreds of users visit MovieLens to rate and receive recommendations for movies. 

The site now has over 43000 users who have expressed opinions on 3500+ different movies. 

We randomly selected enough users to obtain 100; 000 ratings from the database (we only considered users that had rated 20 or more movies). 

We divided the database into a training set and a test set. 

For this purpose, we introduced a variable that determines what percentage of data is used as training and test sets; we call this variable x. 

A value of x = 0:8 would indicate 80% of the data was used as training set and 20% of the data was used as test set. 

The data set was converted into a user-item matrix A that had 943 rows (i.e., 943 users) and 1682 columns (i.e., 1682 movies that were rated by at least one of the users). 

For our experiments, we also take another factor into consideration, sparsity level of data sets. 

For the data matrix R This is defined as 1  nonzero entries total entries . 

The sparsity level of the Movie data set is, therefore, 1 - 100,000/(943*1682) , which is 0:9369.

Throughout the paper we term this data set as ML.

4. ì‹¤í—˜ì  í‰ê°€
4.1 ë°ì´í„° ì„¸íŠ¸

ì—°êµ¬ ì›¹ ì‚¬ì´íŠ¸ì˜ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•­ëª© ê¸°ë°˜ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì˜ ë‹¤ì–‘í•œ ë³€í˜•ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤.

ì˜í™” ë°ì´í„°.

MovieLens ì¶”ì²œ ì‹œìŠ¤í…œì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

MovieLensëŠ” 1997 ë…„ ê°€ì„ì— ë°ë·” í•œ ì›¹ ê¸°ë°˜ ì—°êµ¬ ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ë§¤ì£¼ ìˆ˜ë°± ëª…ì˜ ì‚¬ìš©ìê°€ MovieLensë¥¼ ë°©ë¬¸í•˜ì—¬ ì˜í™”ë¥¼ í‰ê°€í•˜ê³  ì¶”ì²œì„ë°›ìŠµë‹ˆë‹¤.

í˜„ì¬ì´ ì‚¬ì´íŠ¸ì—ëŠ” 3500 ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ì˜í™”ì— ëŒ€í•œ ì˜ê²¬ì„ í‘œëª… í•œ 43000 ëª… ì´ìƒì˜ ì‚¬ìš©ìê°€ ìˆìŠµë‹ˆë‹¤.

100 ëª…ì„ ì–»ê¸°ì— ì¶©ë¶„í•œ ì‚¬ìš©ìë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ì˜ 000 ë“±ê¸‰ (ì˜í™” 20 ê°œ ì´ìƒì˜ ë“±ê¸‰ì„ë°›ì€ ì‚¬ìš©ì ë§Œ ê³ ë ¤).

ìš°ë¦¬ëŠ” ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•´ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì˜ ë¹„ìœ¨ì„ ê²°ì •í•˜ëŠ” ë³€ìˆ˜ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì´ ë³€ìˆ˜ë¥¼ xë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

x = 0 : 8 ê°’ì€ ë°ì´í„°ì˜ 80 %ê°€ í•™ìŠµ ì„¸íŠ¸ë¡œ ì‚¬ìš©ë˜ê³  ë°ì´í„°ì˜ 20 %ê°€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ì‚¬ìš©ë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ë°ì´í„° ì„¸íŠ¸ëŠ” 943 ê°œì˜ í–‰ (ì¦‰, 943 ëª…ì˜ ì‚¬ìš©ì)ê³¼ 1,682 ê°œì˜ ì—´ (ì¦‰, ìµœì†Œ í•œ ëª…ì˜ ì‚¬ìš©ìê°€ í‰ê°€ í•œ 1682 ê°œì˜ ì˜í™”)ì´ìˆëŠ” ì‚¬ìš©ì í•­ëª© í–‰ë ¬ Aë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.

ì‹¤í—˜ì„ ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ì˜ í¬ì†Œì„± ìˆ˜ì¤€ì´ë¼ëŠ” ë˜ ë‹¤ë¥¸ ìš”ì†Œë„ ê³ ë ¤í•©ë‹ˆë‹¤.

ë°ì´í„° í–‰ë ¬ Rì˜ ê²½ìš° ì´ê²ƒì€ 0ì´ ì•„ë‹Œ 1 ê°œì˜ ì´ í•­ëª©ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤.

ë”°ë¼ì„œ Movie ë°ì´í„° ì„¸íŠ¸ì˜ í¬ì†Œì„± ìˆ˜ì¤€ì€ 1-100,000 / (943 * 1682)ì´ë©° 0 : 9369ì…ë‹ˆë‹¤.

ë°±ì„œ ì „ì²´ì—ì„œì´ ë°ì´í„° ì„¸íŠ¸ë¥¼ MLì´ë¼ê³ í•©ë‹ˆë‹¤.






4.2 Evaluation Metrics
Recommender systems research has used several types of measures for evaluating the quality of a recommender system. 

They can be mainly categorized into two classes: 

* Statistical accuracy metrics evaluate the accuracy of a system by comparing the numerical recommendation scores against the actual user ratings for the user-item pairs in the test dataset. 

Mean Absolute Error (MAE) between ratings and predictions is a widely used metric. 

MAE is a measure of the deviation of recommendations from their true user-specified values. 

For each ratings-prediction pair < pi ; qi > this metric treats the absolute error between them, i.e., jpi qi j equally. 

The MAE is computed by first summing these absolute errors of the N corresponding ratings-prediction pairs and then computing the average. 

Formally, MAEê³µì‹ 

The lower the MAE, the more accurately the recommendation engine predicts user ratings. 

Root Mean Squared Error (RMSE), and Correlation are also used as statistical accuracy metric.

* Decision support accuracy metrics evaluate how effective a prediction engine is at helping a user select highquality items from the set of all items. 

These metrics assume the prediction process as a binary operationeither items are predicted (good) or not (bad). 

With this observation, whether a item has a prediction score of 1:5 or 2:5 on a five-point scale is irrelevant if the user only chooses to consider pred most commonly used decision support accuracy metrics are reversal rate, weighted errors and ROC sensitivity [23].

We used MAE as our choice of evaluation metric to report prediction experiments because it is most commonly used and easiest to interpret directly. 

In our previous experiments [23] we have seen that MAE and ROC provide the same ordering of different experimental schemes in terms of prediction quality.

4.2 í‰ê°€ ì§€í‘œ
ì¶”ì²œ ì‹œìŠ¤í…œ ì—°êµ¬ëŠ” ì¶”ì²œ ì‹œìŠ¤í…œì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ìœ í˜•ì˜ ì¸¡ì •ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

ì£¼ë¡œ ë‘ ê°€ì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

* í†µê³„ì  ì •í™•ë„ ë©”íŠ¸ë¦­ì€ ìˆ˜ì¹˜ ì¶”ì²œ ì ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ì˜ ì‚¬ìš©ì í•­ëª© ìŒì— ëŒ€í•œ ì‹¤ì œ ì‚¬ìš©ì ë“±ê¸‰ê³¼ ë¹„êµí•˜ì—¬ ì‹œìŠ¤í…œì˜ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

í‰ì ê³¼ ì˜ˆì¸¡ ì‚¬ì´ì˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE)ëŠ” ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì¸¡ì • í•­ëª©ì…ë‹ˆë‹¤.

MAEëŠ” ì‹¤ì œ ì‚¬ìš©ì ì§€ì • ê°’ì—ì„œ ê¶Œì¥ ì‚¬í•­ì˜ í¸ì°¨ë¥¼ ì¸¡ì • í•œ ê²ƒì…ë‹ˆë‹¤.

ê° ë“±ê¸‰ ì˜ˆì¸¡ ìŒì— ëŒ€í•´ <pi; qi>ì´ ì¸¡ì • í•­ëª©ì€ ë‘˜ ì‚¬ì´ì˜ ì ˆëŒ€ ì˜¤ì°¨, ì¦‰ jpi qi jë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

MAEëŠ” ë¨¼ì € N ê°œì˜ í•´ë‹¹ ë“±ê¸‰-ì˜ˆì¸¡ ìŒì˜ ì ˆëŒ€ ì˜¤ì°¨ë¥¼ í•©í•œ ë‹¤ìŒ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤.

ê³µì‹ì ìœ¼ë¡œ ë§¤ ê³µì‹

MAEê°€ ë‚®ì„ìˆ˜ë¡ ì¶”ì²œ ì—”ì§„ì´ ì‚¬ìš©ì í‰ì ì„ ë” ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

RMSE (Root Mean Squared Error) ë° ìƒê´€ ê´€ê³„ë„ í†µê³„ ì •í™•ë„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

* ì˜ì‚¬ ê²°ì • ì§€ì› ì •í™•ë„ ë©”íŠ¸ë¦­ì€ ì‚¬ìš©ìê°€ ëª¨ë“  í•­ëª© ì§‘í•©ì—ì„œ ê³ í’ˆì§ˆ í•­ëª©ì„ ì„ íƒí•˜ëŠ” ë° ì˜ˆì¸¡ ì—”ì§„ì´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ì¸ì§€ í‰ê°€í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì¸¡ì • í•­ëª©ì€ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ë¥¼ í•­ëª©ì´ ì˜ˆì¸¡ (ì–‘í˜¸)ë˜ê±°ë‚˜ ê·¸ë ‡ì§€ ì•Šì€ (ë‚˜ì¨) ì´ì§„ ì—°ì‚°ìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.

ì´ ê´€ì°°ì„ í†µí•´ í•­ëª©ì˜ ì˜ˆì¸¡ ì ìˆ˜ê°€ 5 ì  ì²™ë„ì—ì„œ 1 : 5ì¸ì§€ 2 : 5ì¸ì§€ ì—¬ë¶€ëŠ” ì‚¬ìš©ìê°€ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì˜ì‚¬ ê²°ì • ì§€ì› ì •í™•ë„ ë©”íŠ¸ë¦­ì´ ë°˜ì „ ë¥ , ê°€ì¤‘ ì˜¤ë¥˜ ë° ROCë¥¼ ê³ ë ¤í•˜ë„ë¡ ì„ íƒí•˜ëŠ” ê²½ìš° ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ê°ë„ [23].

MAEëŠ” ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ì§ì ‘ í•´ì„í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ ì‹¤í—˜ì„ë³´ê³ í•˜ê¸°ìœ„í•œ í‰ê°€ ì¸¡ì • í•­ëª©ìœ¼ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤.

ì´ì „ ì‹¤í—˜ [23]ì—ì„œ MAEì™€ ROCê°€ ì˜ˆì¸¡ í’ˆì§ˆ ì¸¡ë©´ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì‹¤í—˜ ê³„íšì˜ ë™ì¼í•œ ìˆœì„œë¥¼ ì œê³µí•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.







4.2.1 Experimental Procedure

Experimental steps. 

We started our experiments by first dividing the data set into a training and a test portion. 

Before starting full experimental evaluation of different algorithms we determined the sensitivity of different parameters to different algorithms and from the sensitivity plots we fixed the optimum values of these parameters and used them for the rest of the experiments. 

To determine the parameter sensitivity, we work only with the training data and further subdivide it into a training and test portion and carried on our experiments on them. 

For conducted a 10-fold cross validation of our experiments by randomly choosing different training and test sets each time and taking the average of the MAE values.

Benchmark user-based system. 

To compare the performance of item-based prediction we also entered the training ratings set into a collaborative filtering recommendation engine that employs the Pearson nearest neighbor algorithm (user-user). 

For this purpose we implemented a exible prediction engine that implements user-based CF algorithms.

We tuned the algorithm to use the best published Pearson nearest neighbor algorithm and configured it to deliver the highest quality prediction without concern for performance (i.e., it considered every possible neighbor to form optimal neighborhoods).

Experimental platform. 

All our experiments were implemented using C and compiled using optimization  flag-06.

We ran all our experiments on a linux based PC with Intel Pentium III processor having a speed of 600 MHz and 2GB of RAM.

4.2.1 ì‹¤í—˜ ì ˆì°¨

ì‹¤í—˜ ë‹¨ê³„.

ë¨¼ì € ë°ì´í„° ì„¸íŠ¸ë¥¼ í›ˆë ¨ ë¶€ë¶„ê³¼ í…ŒìŠ¤íŠ¸ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ë©´ì„œ ì‹¤í—˜ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.

ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì „ì²´ ì‹¤í—˜ í‰ê°€ë¥¼ ì‹œì‘í•˜ê¸° ì „ì— ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì„œë¡œ ë‹¤ë¥¸ ë§¤ê°œ ë³€ìˆ˜ì˜ ë¯¼ê°ë„ë¥¼ ê²°ì •í–ˆê³  ë¯¼ê°ë„ í”Œë¡¯ì—ì„œ ì´ëŸ¬í•œ ë§¤ê°œ ë³€ìˆ˜ì˜ ìµœì  ê°’ì„ ê³ ì •í•˜ê³  ë‚˜ë¨¸ì§€ ì‹¤í—˜ì— ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

ë§¤ê°œ ë³€ìˆ˜ ë¯¼ê°ë„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” í›ˆë ¨ ë°ì´í„°ë¡œë§Œ ì‘ì—…í•˜ê³ ì´ë¥¼ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë¶€ë¶„ìœ¼ë¡œ ë” ì„¸ë¶„í™”í•˜ê³  ì´ì— ëŒ€í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

ë§¤ë²ˆ ë‹¤ë¥¸ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ê³  MAE ê°’ì˜ í‰ê· ì„ ì·¨í•˜ì—¬ ì‹¤í—˜ì˜ 10 ë°° êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

ì‚¬ìš©ì ê¸°ë°˜ ì‹œìŠ¤í…œì„ ë²¤ì¹˜ ë§ˆí¬í•©ë‹ˆë‹¤.

í•­ëª© ê¸°ë°˜ ì˜ˆì¸¡ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ Pearson ìµœê·¼ ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ (ì‚¬ìš©ì-ì‚¬ìš©ì)ì„ ì‚¬ìš©í•˜ëŠ” í˜‘ì—… í•„í„°ë§ ê¶Œì¥ ì—”ì§„ì— ì„¤ì •ëœ í›ˆë ¨ ë“±ê¸‰ë„ ì…ë ¥í–ˆìŠµë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•´ ì‚¬ìš©ì ê¸°ë°˜ CF ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì—”ì§„ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ëŠ” ê°€ì¥ ì˜ ê²Œì‹œ ëœ Pearson ìµœê·¼ ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ë„ë¡ ì•Œê³ ë¦¬ì¦˜ì„ ì¡°ì •í•˜ê³  ì„±ëŠ¥ì— ëŒ€í•œ ê±±ì •ì—†ì´ ìµœê³  í’ˆì§ˆì˜ ì˜ˆì¸¡ì„ ì œê³µí•˜ë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤ (ì¦‰, ê°€ëŠ¥í•œ ëª¨ë“  ì´ì›ƒì´ ìµœì ì˜ ì´ì›ƒì„ í˜•ì„±í•˜ë„ë¡ ê³ ë ¤í•¨).

ì‹¤í—˜ì  í”Œë«í¼.

ëª¨ë“  ì‹¤í—˜ì€ Cë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆê³  ìµœì í™” í”Œë˜ê·¸ -06ì„ ì‚¬ìš©í•˜ì—¬ ì»´íŒŒì¼ë˜ì—ˆìŠµë‹ˆë‹¤.

ëª¨ë“  ì‹¤í—˜ì€ ì†ë„ê°€ 600MHzì´ê³  RAMì´ 2GB ì¸ Intel Pentium III í”„ë¡œì„¸ì„œê°€ìˆëŠ” Linux ê¸°ë°˜ PCì—ì„œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.







4.3 Experimental Results
In this section we present our experimental results of applying item-based collaborative filtering techniques for generating predictions. 

Our results are mainly divided into two parts|quality results and performance results. 

In assessing the quality of recommendations, we first determined the sensitivity of some parameters before running the main experiment. 

These parameters include the neighborhood size, the value of the training/test ratio x, and effects of different similarity measures. 

For determining the sensitivity of various parameters, we focused only on the training data set and further divided it into a training and a test portion and used them to learn the parameters.

4.3.1 Effect of Similarity Algorithms
We implemented three different similarity algorithms basic cosine, adjusted cosine and correlation as described in Section 3.1 and tested them on our data sets. 

For each similarity algorithms, we implemented the algorithm to compute the neighborhood and used weighted sum algorithm to generate the prediction. 

We ran these experiments on our training data and used test set to compute Mean Absolute Error (MAE). 

Figure 4 shows the experimental results. 

It can be observed from the results that offsetting the user-average for cosine similarity computation has a clear advantage, as the MAE is significantly lower in this case. 

Hence, we select the adjusted cosine similarity for the rest of our experiments.

4.3.2 Sensitivity of Training/Test Ratio
To determine the sensitivity of density of the data set, we carried out an experiment where we varied the value of x from 0:2 to 0:9 in an increment of 0:1. 

For each of these training/test ratio values we ran our experiments using the two prediction generation techniques{basic weighted sum and regression based approach. 

Our results are shown in Figure 5. 

We observe that the quality of prediction increase as we increase x. 

The regression-based approach shows better results than the basic scheme for low values of x but as we increase x the quality tends to fall below the basic scheme. 

From the curves, we select x = 0:8 as an optimum value for our subsequent experiments.


4.3 ì‹¤í—˜ ê²°ê³¼
ì´ ì„¹ì…˜ì—ì„œëŠ” ì˜ˆì¸¡ ìƒì„±ì„ ìœ„í•´ í•­ëª© ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ê¸°ìˆ ì„ ì ìš©í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ì£¼ë¡œ ë‘ ë¶€ë¶„ì˜ í’ˆì§ˆ ê²°ê³¼ì™€ ì„±ëŠ¥ ê²°ê³¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

ê¶Œì¥ ì‚¬í•­ì˜ í’ˆì§ˆì„ í‰ê°€í•  ë•Œ ë¨¼ì € ì£¼ìš” ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê¸° ì „ì— ì¼ë¶€ ë§¤ê°œ ë³€ìˆ˜ì˜ ë¯¼ê°ë„ë¥¼ ê²°ì •í–ˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ë§¤ê°œ ë³€ìˆ˜ì—ëŠ” ì´ì›ƒ í¬ê¸°, í›ˆë ¨ / ê²€ì • ë¹„ìœ¨ xì˜ ê°’ ë° ë‹¤ì–‘í•œ ìœ ì‚¬ì„± ì¸¡ì •ì˜ íš¨ê³¼ê°€ í¬í•¨ë©ë‹ˆë‹¤.

ë‹¤ì–‘í•œ ë§¤ê°œ ë³€ìˆ˜ì˜ ë¯¼ê°ë„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì—ë§Œ ì§‘ì¤‘í•˜ê³ ì´ë¥¼ í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë¶€ë¶„ìœ¼ë¡œ ë” ë‚˜ëˆ„ê³  ë§¤ê°œ ë³€ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

4.3.1 ìœ ì‚¬ì„± ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼
3.1 ì ˆì— ì„¤ëª… ëœëŒ€ë¡œ ì„¸ ê°€ì§€ ìœ ì‚¬ì„± ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ë³¸ ì½”ì‚¬ì¸, ì¡°ì • ëœ ì½”ì‚¬ì¸ ë° ìƒê´€ ê´€ê³„ë¥¼ êµ¬í˜„í•˜ê³  ë°ì´í„° ì„¸íŠ¸ì—ì„œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤.

ê° ìœ ì‚¬ì„± ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì´ì›ƒì„ ê³„ì‚°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê³  ê°€ì¤‘ í•©ê³„ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

í›ˆë ¨ ë°ì´í„°ì—ì„œ ì´ëŸ¬í•œ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê³  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ˆëŒ€ í‰ê·  ì˜¤ì°¨ (MAE)ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.

ê·¸ë¦¼ 4ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì´ ê²½ìš° MAEê°€ í˜„ì €íˆ ë‚®ê¸° ë•Œë¬¸ì— ì½”ì‚¬ì¸ ìœ ì‚¬ì„± ê³„ì‚°ì— ëŒ€í•œ ì‚¬ìš©ì í‰ê· ì„ ìƒì‡„í•˜ëŠ” ê²ƒì´ ë¶„ëª…í•œ ì´ì ì´ ìˆìŒì„ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë”°ë¼ì„œ ë‚˜ë¨¸ì§€ ì‹¤í—˜ì— ëŒ€í•´ ì¡°ì • ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ ì„ íƒí•©ë‹ˆë‹¤.

4.3.2 í›ˆë ¨ / í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ì˜ ë¯¼ê°ë„
ë°ì´í„° ì„¸íŠ¸ì˜ ë°€ë„ ë¯¼ê°ë„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ x ê°’ì„ 0 : 1 ì”© 0 : 2ì—ì„œ 0 : 9ë¡œ ë³€ê²½í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ê° í•™ìŠµ / í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ ê°’ì— ëŒ€í•´ ë‘ ê°€ì§€ ì˜ˆì¸¡ ìƒì„± ê¸°ìˆ  (ê¸°ë³¸ ê°€ì¤‘ í•©ê³„ ë° íšŒê·€ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹)ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.

ê²°ê³¼ëŠ” ê·¸ë¦¼ 5ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.

xê°€ ì¦ê°€í• ìˆ˜ë¡ ì˜ˆì¸¡ì˜ ì§ˆì´ ì¦ê°€í•˜ëŠ” ê²ƒì„ ê´€ì°°í•©ë‹ˆë‹¤.

íšŒê·€ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì€ xì˜ ë‚®ì€ ê°’ì— ëŒ€í•œ ê¸°ë³¸ ê³„íšë³´ë‹¤ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ ì£¼ì§€ë§Œ xë¥¼ ì¦ê°€ ì‹œí‚¤ë©´ í’ˆì§ˆì´ ê¸°ë³¸ ê³„íš ì•„ë˜ë¡œ ë–¨ì–´ì§€ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

ê³¡ì„ ì—ì„œ x = 0 : 8ì„ í›„ì† ì‹¤í—˜ì— ëŒ€í•œ ìµœì  ê°’ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.








4.3.3 Experiments with neighborhood size
The size of the neighborhood has significant impact on the prediction quality [12]. 

To determine the sensitivity of this parameter, we performed an experiment where we varied the number of neighbors to be used and computed MAE.

Our results are shown in Figure 5. 

We can observe that the size of neighborhood does affect the quality of prediction. 

But the two methods show different types of sensitivity. 

The basic item-item algorithm improves as we increase the neighborhood size from 10 to 30, after that the rate of increase diminishes and the curve tends to be at. 

On the other hand, the regression-based algorithm shows decrease in prediction quality with increased number of neighbors.

Considering both trends we select 30 as our optimal choice of neighborhood size.

4.3.3 ì´ì›ƒ í¬ê¸° ì‹¤í—˜
ì´ì›ƒì˜ í¬ê¸°ëŠ” ì˜ˆì¸¡ í’ˆì§ˆì— ìƒë‹¹í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤ [12].

ì´ ë§¤ê°œ ë³€ìˆ˜ì˜ ë¯¼ê°ë„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ì´ì›ƒ ìˆ˜ë¥¼ ë³€ê²½í•˜ê³  MAEë¥¼ ê³„ì‚°í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

ê²°ê³¼ëŠ” ê·¸ë¦¼ 5ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.

ì´ì›ƒì˜ í¬ê¸°ê°€ ì˜ˆì¸¡ì˜ ì§ˆì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ë‘ ê°€ì§€ ë°©ë²•ì€ ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ê°ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

ê¸°ë³¸ í•­ëª©-í•­ëª© ì•Œê³ ë¦¬ì¦˜ì€ ì´ì›ƒ í¬ê¸°ë¥¼ 10ì—ì„œ 30ìœ¼ë¡œ ëŠ˜ë¦¬ë©´ ì¦ê°€ìœ¨ì´ ê°ì†Œí•˜ê³  ê³¡ì„ ì´ë˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

ë°˜ë©´ íšŒê·€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ì´ì›ƒ ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì˜ˆì¸¡ í’ˆì§ˆì´ ì €í•˜ë˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ë‘ ê°€ì§€ ì¶”ì„¸ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ 30ì„ ìµœì ì˜ ì´ì›ƒ í¬ê¸°ë¡œ ì„ íƒí•©ë‹ˆë‹¤.





4.3.4 Quality Experiments
Once we obtain the optimal values of the parameters, we compare both of our item-based approaches with the benchmark user-based algorithm. 

We present the results in Figure 6. 

It can be observed from the charts that the basic item-item algorithm out performs the user based algorithm at all values of x (neighborhood size = 30) and all values of neighborhood size (x = 0:8). 

For example, at x = 0:5 user-user scheme has an MAE of 0:755 and item-item scheme shows an MAE of 0:749. 

Similarly at a neighborhood size of 60 user-user and item-item schemes show MAE of 0:732 and 0:726 respectively. 

The regression-based algorithm, however, shows interesting behavior. 

At low values of x and at low neighborhood size it out performs the other two algorithms, but as the density of the data set is increased or as we add more neighbors it performs worse, even compared to the user-based algorithm. 

We also compared our algorithms against the naive nonpersonalized algorithm described in [12].

We draw two conclusions from these results. 

First, itembased algorithms provide better quality than the user-based algorithms at all sparsity levels. 

Second, regression-based algorithms perform better with very sparse data set, but as we add more data the quality goes down. 

We believe this happens as the regression model suffers from data overfitting at high density levels.

4.3.4 í’ˆì§ˆ ì‹¤í—˜
ë§¤ê°œ ë³€ìˆ˜ì˜ ìµœì  ê°’ì„ ì–»ì€ í›„ì—ëŠ” ë‘ í•­ëª© ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ë²¤ì¹˜ ë§ˆí¬ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµí•©ë‹ˆë‹¤.

ê²°ê³¼ëŠ” ê·¸ë¦¼ 6ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.

ê¸°ë³¸ í•­ëª©-í•­ëª© ì•Œê³ ë¦¬ì¦˜ ì¶œë ¥ì€ x (ì´ì›ƒ í¬ê¸° = 30)ì˜ ëª¨ë“  ê°’ê³¼ ì´ì›ƒ í¬ê¸° (x = 0 : 8)ì˜ ëª¨ë“  ê°’ì—ì„œ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ì°¨íŠ¸ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, at x = 0 : 5 ì‚¬ìš©ì-ì‚¬ìš©ì ì²´ê³„ëŠ” MAEê°€ 0 : 755ì´ê³  í•­ëª©-í•­ëª© ì²´ê³„ëŠ” MAEê°€ 0 : 749ì…ë‹ˆë‹¤.

ìœ ì‚¬í•˜ê²Œ 60 ê°œì˜ ì‚¬ìš©ì-ì‚¬ìš©ì ë° í•­ëª©-í•­ëª© ì²´ê³„ì˜ ì´ì›ƒ í¬ê¸°ì—ì„œ MAEëŠ” ê°ê° 0 : 732 ë° 0 : 726ì…ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ íšŒê·€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ í¥ë¯¸ë¡œìš´ ë™ì‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

x ê°’ì´ ë‚®ê³  ì´ì›ƒ í¬ê¸°ê°€ ë‚® ìœ¼ë©´ ë‹¤ë¥¸ ë‘ ì•Œê³ ë¦¬ì¦˜ì„ ìˆ˜í–‰í•˜ì§€ë§Œ ë°ì´í„° ì„¸íŠ¸ì˜ ë°€ë„ê°€ ì¦ê°€í•˜ê±°ë‚˜ ì´ì›ƒì„ ë” ì¶”ê°€í•˜ë©´ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì— ë¹„í•´ ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤.

ë˜í•œ ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì„ [12]ì— ì„¤ëª… ëœ ìˆœì§„í•œ ë¹„ ê°œì¸í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµí–ˆìŠµë‹ˆë‹¤.

ì´ ê²°ê³¼ì—ì„œ ë‘ ê°€ì§€ ê²°ë¡ ì„ ë„ì¶œí•©ë‹ˆë‹¤.

ì²«ì§¸, í•­ëª© ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ëª¨ë“  í¬ì†Œì„± ìˆ˜ì¤€ì—ì„œ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ë‚˜ì€ í’ˆì§ˆì„ ì œê³µí•©ë‹ˆë‹¤.

ë‘˜ì§¸, íšŒê·€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ë§¤ìš° í¬ì†Œ í•œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë” ì˜ ìˆ˜í–‰ë˜ì§€ë§Œ ë” ë§ì€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ë©´ í’ˆì§ˆì´ ì €í•˜ë©ë‹ˆë‹¤.

íšŒê·€ ëª¨ë¸ì´ ê³ ë°€ë„ ìˆ˜ì¤€ì—ì„œ ë°ì´í„° ê³¼ì  í•©ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªê¸° ë•Œë¬¸ì— ì´ëŸ° ì¼ì´ ë°œìƒí•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.






4.3.5 Performance Results
After showing that the item-based algorithm provides better quality of prediction than the user-based algorithm, we focus on the scalability issues. 

As discussed earlier, itembased similarity is more static and allows us to precompute the item neighborhood. 

This precomputation of the model has certain performance benefits. 

To make the system even more scalable we looked into the sensitivity of the model size and then looked into the impact of model size on the response time and throughput.


4.3.5 ì„±ëŠ¥ ê²°ê³¼
í•­ëª© ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ì ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ë‚˜ì€ ì˜ˆì¸¡ í’ˆì§ˆì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤€ í›„ í™•ì¥ ì„± ë¬¸ì œì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.

ì•ì„œ ë…¼ì˜í–ˆë“¯ì´ í•­ëª© ê¸°ë°˜ ìœ ì‚¬ì„±ì€ ë” ì •ì ì´ê³  í•­ëª© ì´ì›ƒì„ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ëª¨ë¸ì˜ ì‚¬ì „ ê³„ì‚°ì—ëŠ” íŠ¹ì • ì„±ëŠ¥ ì´ì ì´ ìˆìŠµë‹ˆë‹¤.

ì‹œìŠ¤í…œì˜ í™•ì¥ ì„±ì„ ë”ìš± ë†’ì´ê¸° ìœ„í•´ ëª¨ë¸ í¬ê¸°ì˜ ë¯¼ê°ë„ë¥¼ ì¡°ì‚¬í•œ ë‹¤ìŒ ëª¨ë¸ í¬ê¸°ê°€ ì‘ë‹µ ì‹œê°„ ë° ì²˜ë¦¬ëŸ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.





4.4 Sensitivity of the Model Size
To experimentally determine the impact of the model size on the quality of the prediction, we selectively varied the number of items to be used for similarity computation from 25 to 200 in an increment of 25. 

A model size of l means that we only considered l best similarity values for model building and later used k of them for the prediction generation process, where k < l. 

Using the training data set we precomputed the item similarity using different model sizes and then used only the weighted sum prediction generation technique to provide the predictions. 

We then used the test data set to compute MAE and plotted the values. 

To compare with the full model size (i.e., model size = no. of items) we also ran the same test considering all similarity values and picked best k for prediction generation. 

We repeated the entire process for three different x values (training/test ratios). 

Figure 7 shows the plots at different x values. 

It can be observed from the plots that the MAE values get better as we increase the model size and the improvements are drastic at the beginning, but gradually slow down as we increase the model size. 

The most important observation from these plots is the high accuracy that can be achieved using only a fraction of items. 

For example, at x = 0:3 the full item-item scheme provided an MAE of 0:7873, but using a model size of only 25, we were able to achieve an MAE value of 0:842. 

At x = 0:8 these numbers are even more appealing|for the full item-item we had an MAE of 0:726 but using a model size of only 25 we were able to obtain an MAE of 0:754, and using a model size of 50 the MAE was 0:738. 

In other words, at x = 0:8 we were within 96% and 98:3% of the full item-item scheme's accuracy using only 1.9% and 3% of the items, respectively!

This model size sensitivity has important performance implications. 

It appears from the plots that it is useful to precompute the item similarities using only a fraction of items and yet possible to obtain good prediction quality.

4.4 ëª¨ë¸ í¬ê¸°ì˜ ë¯¼ê°ë„
ëª¨ë¸ í¬ê¸°ê°€ ì˜ˆì¸¡ í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•˜ê¸° ìœ„í•´ ìœ ì‚¬ì„± ê³„ì‚°ì— ì‚¬ìš©í•  í•­ëª© ìˆ˜ë¥¼ 25 ê°œì—ì„œ 200 ê°œì”© 25 ê°œì”© ì„ íƒì ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.

ëª¨ë¸ í¬ê¸°ê°€ lì´ë¼ëŠ” ê²ƒì€ ëª¨ë¸ êµ¬ì¶•ì„ ìœ„í•´ l ê°œì˜ ìµœìƒì˜ ìœ ì‚¬ì„± ê°’ë§Œ ê³ ë ¤í•˜ê³  ë‚˜ì¤‘ì— ì˜ˆì¸¡ ìƒì„± í”„ë¡œì„¸ìŠ¤ì— k <lì„ ì‚¬ìš©í–ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•­ëª© ìœ ì‚¬ì„±ì„ ë¯¸ë¦¬ ê³„ì‚° í•œ ë‹¤ìŒ ê°€ì¤‘ í•©ê³„ ì˜ˆì¸¡ ìƒì„± ê¸°ìˆ  ë§Œ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ MAEë¥¼ ê³„ì‚°í•˜ê³  ê°’ì„ í”Œë¡œíŒ…í–ˆìŠµë‹ˆë‹¤.

ì „ì²´ ëª¨ë¸ í¬ê¸° (ì¦‰, ëª¨ë¸ í¬ê¸° = í•­ëª© ìˆ˜)ì™€ ë¹„êµí•˜ê¸° ìœ„í•´ ëª¨ë“  ìœ ì‚¬ì„± ê°’ì„ ê³ ë ¤í•˜ì—¬ ë™ì¼í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ì˜ˆì¸¡ ìƒì„±ì„ ìœ„í•´ ìµœìƒì˜ kë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.

ì„¸ ê°€ì§€ x ê°’ (í›ˆë ¨ / í…ŒìŠ¤íŠ¸ ë¹„ìœ¨)ì— ëŒ€í•´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ë°˜ë³µí–ˆìŠµë‹ˆë‹¤.

ê·¸ë¦¼ 7ì€ ì„œë¡œ ë‹¤ë¥¸ x ê°’ì˜ í”Œë¡¯ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

í”Œë¡¯ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ëª¨ë¸ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ë©´ MAE ê°’ì´ ì¢‹ì•„ì§€ê³  ì²˜ìŒì—ëŠ” ê°œì„ ì´ ê³¼ê°í•˜ì§€ë§Œ ëª¨ë¸ í¬ê¸°ë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ ì ì°¨ ëŠë ¤ì§‘ë‹ˆë‹¤.

ì´ í”Œë¡¯ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê´€ì°°ì€ í•­ëª©ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ì—¬ ì–»ì„ ìˆ˜ìˆëŠ” ë†’ì€ ì •í™•ë„ì…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, x = 0 : 3ì—ì„œ ì „ì²´ í•­ëª©-í•­ëª© ì²´ê³„ëŠ” 0 : 7873ì˜ MAEë¥¼ ì œê³µí–ˆì§€ë§Œ ëª¨ë¸ í¬ê¸°ê°€ 25 ì¸ ê²½ìš° MAE ê°’ 0 : 842ë¥¼ ì–»ì„ ìˆ˜ìˆì—ˆìŠµë‹ˆë‹¤.

x = 0 : 8ì—ì„œì´ ìˆ«ìëŠ” ì „ì²´ í•­ëª© í•­ëª©ì— ëŒ€í•´ í›¨ì”¬ ë” ë§¤ë ¥ì ì…ë‹ˆë‹¤ | ìš°ë¦¬ëŠ” 0 : 726ì˜ MAEë¥¼ ê°€ì¡Œì§€ ë§Œ 25ì˜ ëª¨ë¸ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ 0 : 754ì˜ MAEë¥¼ ì–»ì„ ìˆ˜ìˆì—ˆìŠµë‹ˆë‹¤. 50ì˜ ëª¨ë¸ í¬ê¸° MAEëŠ” 0 : 738ì…ë‹ˆë‹¤.

ì¦‰, x = 0 : 8ì—ì„œ ìš°ë¦¬ëŠ” ì•„ì´í…œì˜ 1.9 %ì™€ 3 %ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì•„ì´í…œ-ì•„ì´í…œ ì²´ê³„ì˜ ì •í™•ë„ì˜ 96 %ì™€ 98 : 3 % ì´ë‚´ì˜€ìŠµë‹ˆë‹¤!

ì´ ëª¨ë¸ í¬ê¸° ë¯¼ê°ë„ëŠ” ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.

í•­ëª©ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ì—¬ í•­ëª© ìœ ì‚¬ì„±ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ìœ ìš©í•˜ì§€ë§Œ ì¢‹ì€ ì˜ˆì¸¡ í’ˆì§ˆì„ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ í”Œë¡¯ì—ì„œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.





4.4.1 Impact of the model size on run-time and throughput
Given the quality of prediction is reasonably good with small model size, we focus on the run-time and throughput of the system. 

We recorded the time required to generate predictions for the entire test set and plotted them in a chart with varying model size. 

We plotted the run time at different x values. 

Figure 8 shows the plot. 

Note here that at x = 0:25 the whole system has to make prediction for 25; 000 test cases. 

From the plot we observe a substantial difference in the run-time between the small model size and the full item-item prediction case. 

For x = 0:25 the run-time is 2:002 seconds for a model size of 200 as opposed to 14:11 for the basic item-item case. 

This difference is even more prominent with x = 0:8 where a model size of 200 requires only 1:292 seconds and the basic item-item case requires 36:34 seconds.

These run-time numbers may be misleading as we computed them for different training/test ratios where the workload size, i.e., number of predictions to be generated is different (recall that at x = 0:3 our algorithm uses 30; 000 ratings as training data and uses the rest of 70; 000 ratings as test data to compare predictions generated by the system to the actual ratings). 

To make the numbers comparable we compute the throughput (predictions generated per second) for the model based and basic item-item schemes. 

Figure 8 charts these results. 

We see that for x = 0:3 and at a model size of 100 the system generates 70; 000 ratings in 1:487 seconds producing a throughput rate of 47; 361 where as the basic item-item scheme produced a throughput of 4961 only. 

At x = 0:8 these two numbers are 21; 505 and 550 respectively.

4.4.1 ëŸ°íƒ€ì„ ë° ì²˜ë¦¬ëŸ‰ì— ëŒ€í•œ ëª¨ë¸ í¬ê¸°ì˜ ì˜í–¥
ëª¨ë¸ í¬ê¸°ê°€ ì‘ì„ìˆ˜ë¡ ì˜ˆì¸¡ í’ˆì§ˆì´ ìƒë‹¹íˆ ì¢‹ìœ¼ë¯€ë¡œ ì‹œìŠ¤í…œì˜ ëŸ°íƒ€ì„ê³¼ ì²˜ë¦¬ëŸ‰ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.

ì „ì²´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ì‹œê°„ì„ ê¸°ë¡í•˜ê³  ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨íŠ¸ì— í‘œì‹œí–ˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ëŠ” ë‹¤ë¥¸ x ê°’ì—ì„œ ëŸ°íƒ€ì„ì„ í”Œë¡œíŒ…í–ˆìŠµë‹ˆë‹¤.

ê·¸ë¦¼ 8ì€ í”Œë¡¯ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì—¬ê¸°ì„œ x = 0:25ì—ì„œ ì „ì²´ ì‹œìŠ¤í…œì€ 25ì— ëŒ€í•œ ì˜ˆì¸¡ì„í•´ì•¼í•©ë‹ˆë‹¤. 000 ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤.

í”Œë¡¯ì—ì„œ ìš°ë¦¬ëŠ” ì‘ì€ ëª¨ë¸ í¬ê¸°ì™€ ì „ì²´ í•­ëª©-í•­ëª© ì˜ˆì¸¡ ì¼€ì´ìŠ¤ ê°„ì˜ ëŸ°íƒ€ì„ì—ì„œ ìƒë‹¹í•œ ì°¨ì´ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤.

x = 0:25ì˜ ê²½ìš° ëŸ°íƒ€ì„ì€ ê¸°ë³¸ í•­ëª©-í•­ëª© ì¼€ì´ìŠ¤ì˜ ê²½ìš° 14:11ê³¼ ë‹¬ë¦¬ ëª¨ë¸ í¬ê¸° 200ì˜ ê²½ìš° 2 : 002 ì´ˆì…ë‹ˆë‹¤.

ì´ ì°¨ì´ëŠ” x = 0 : 8ì—ì„œ ë”ìš± ë‘ë“œëŸ¬ì§€ë©° 200ì˜ ëª¨ë¸ í¬ê¸°ëŠ” 1 : 292 ì´ˆë§Œ í•„ìš”í•˜ê³  ê¸°ë³¸ í•­ëª©-í•­ëª© ì¼€ì´ìŠ¤ëŠ” 36:34 ì´ˆê°€ í•„ìš”í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ëŸ°íƒ€ì„ ìˆ˜ì¹˜ëŠ” ì›Œí¬ë¡œë“œ í¬ê¸°, ì¦‰ ìƒì„± ë  ì˜ˆì¸¡ ìˆ˜ê°€ ë‹¤ë¥¸ ì—¬ëŸ¬ í›ˆë ¨ / í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ì— ëŒ€í•´ ê³„ì‚°í–ˆê¸° ë•Œë¬¸ì— ì˜¤í•´ì˜ ì†Œì§€ê°€ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (x = 0 : 3ì—ì„œ ìš°ë¦¬ ì•Œê³ ë¦¬ì¦˜ì€ 30ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì—ì„œ ìƒì„± ëœ ì˜ˆì¸¡ì„ ì‹¤ì œ í‰ê°€ì™€ ë¹„êµí•˜ê¸°ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë‚˜ë¨¸ì§€ 70, 000 ê°œì˜ í‰ê°€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ìˆ˜ì¹˜ë¥¼ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ ê¸°ë°˜ ë° ê¸°ë³¸ í•­ëª©-í•­ëª© ì²´ê³„ì— ëŒ€í•œ ì²˜ë¦¬ëŸ‰ (ì´ˆë‹¹ ìƒì„± ëœ ì˜ˆì¸¡)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

ê·¸ë¦¼ 8ì€ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

x = 0 : 3ì´ê³  ëª¨ë¸ í¬ê¸°ê°€ 100 ì¼ ë•Œ ì‹œìŠ¤í…œì€ 70ì„ ìƒì„±í•©ë‹ˆë‹¤. 1 : 487 ì´ˆì— 000 ë“±ê¸‰ìœ¼ë¡œ 47ì˜ ì²˜ë¦¬ ì†ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 361 ì—¬ê¸°ì„œ ê¸°ë³¸ í•­ëª©-í•­ëª© ì²´ê³„ëŠ” 4961ì˜ ì²˜ë¦¬ëŸ‰ ë§Œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

x = 0 : 8ì—ì„œì´ ë‘ ìˆ«ìëŠ” 21ì…ë‹ˆë‹¤. ê°ê° 505ì™€ 550.







4.5 Discussion
From the experimental evaluation of the item-item collaborative filtering scheme we make some important observations. 

First, the item-item scheme provides better quality of predictions than the use-user (k-nearest neighbor) scheme.

The improvement in quality is consistent over different neighborhood size and training/test ratio. 

However, the improvement is not significantly large. 

The second observation is that the item neighborhood is fairly static, which can be potentially pre-computed, which results in very high online performance. 

Furthermore, due to the model-based approach, it is possible to retain only a small subset of items and produce reasonably good prediction quality. 

Our experimental results support that claim. 

Therefore, the itemitem scheme is capable in addressing the two most important challenges of recommender systems for E-Commerce{quality of prediction and high performance.


4.5 í† ë¡ 
í•­ëª©-í•­ëª© í˜‘ì—… í•„í„°ë§ ì²´ê³„ì˜ ì‹¤í—˜ì  í‰ê°€ì—ì„œ ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ê´€ì°°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì²«ì§¸, í•­ëª©-í•­ëª© ì²´ê³„ëŠ” ì‚¬ìš© ì‚¬ìš©ì (k- ìµœê·¼ ì ‘ ì´ì›ƒ) ì²´ê³„ë³´ë‹¤ ë” ë‚˜ì€ ì˜ˆì¸¡ í’ˆì§ˆì„ ì œê³µí•©ë‹ˆë‹¤.

í’ˆì§ˆ í–¥ìƒì€ ì´ì›ƒ í¬ê¸°ì™€ í›ˆë ¨ / í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ì— ë”°ë¼ ì¼ê´€ë©ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ê°œì„ ì€ í¬ê²Œ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤.

ë‘ ë²ˆì§¸ ê´€ì°°ì€ í•­ëª© ì´ì›ƒì´ ìƒë‹¹íˆ ì •ì ì´ê³  ì ì¬ì ìœ¼ë¡œ ì‚¬ì „ ê³„ì‚° ë  ìˆ˜ìˆì–´ ì˜¨ë¼ì¸ ì„±ëŠ¥ì´ ë§¤ìš° ë†’ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë˜í•œ ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì¸í•´ í•­ëª©ì˜ ì‘ì€ í•˜ìœ„ ì§‘í•© ë§Œ ìœ ì§€í•˜ê³  í•©ë¦¬ì ìœ¼ë¡œ ì¢‹ì€ ì˜ˆì¸¡ í’ˆì§ˆì„ ìƒì„± í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ì˜ ì‹¤í—˜ ê²°ê³¼ëŠ” ê·¸ ì£¼ì¥ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ itemitem ì²´ê³„ëŠ” ì „ì ìƒê±°ë˜ë¥¼ìœ„í•œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ê°€ì¥ ì¤‘ìš”í•œ ë‘ ê°€ì§€ ë¬¸ì œ (ì˜ˆì¸¡ í’ˆì§ˆ ë° ê³ ì„±ëŠ¥)ë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.







5. CONCLUSION
Recommender systems are a powerful new technology for extracting additional value for a business from its user databases. 

These systems help users find items they want to buy from a business. 

Recommender systems benefit users by enabling them to find items they like. 

Conversely, they help the business by generating more sales. 

Recommender systems are rapidly becoming a crucial tool in E-commerce on the Web. 

Recommender systems are being stressed by the huge volume of user data in existing corporate databases, and will be stressed even more by the increasing volume of user data available on the Web. 

New technologies are needed that can dramatically improve the scalability of recommender systems.

In this paper we presented and experimentally evaluated a new algorithm for CF-based recommender systems. 

Our results show that item-based techniques hold the promise of allowing CF-based algorithms to scale to large data sets and at the same time produce high-quality recommendations.

5. ê²°ë¡ 
Recommender ì‹œìŠ¤í…œì€ ì‚¬ìš©ì ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ì— ëŒ€í•œ ì¶”ê°€ ê°€ì¹˜ë¥¼ ì¶”ì¶œí•˜ê¸°ìœ„í•œ ê°•ë ¥í•œ ì‹ ê¸°ìˆ ì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ë¹„ì¦ˆë‹ˆìŠ¤ì—ì„œ êµ¬ë§¤í•˜ë ¤ëŠ” í•­ëª©ì„ ì°¾ëŠ” ë° ë„ì›€ì´ë©ë‹ˆë‹¤.

ì¶”ì²œ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ì¢‹ì•„í•˜ëŠ” í•­ëª©ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ í˜œíƒì„ì¤ë‹ˆë‹¤.

ë°˜ëŒ€ë¡œ, ê·¸ë“¤ì€ ë” ë§ì€ íŒë§¤ë¥¼ ìƒì„±í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ë¥¼ ë•ìŠµë‹ˆë‹¤.

ì¶”ì²œ ì‹œìŠ¤í…œì€ ì›¹ì—ì„œ ì „ì ìƒê±°ë˜ì—ì„œ ì¤‘ìš”í•œ ë„êµ¬ê°€ë˜ê³  ìˆìŠµë‹ˆë‹¤.

Recommender ì‹œìŠ¤í…œì€ ê¸°ì¡´ ê¸°ì—… ë°ì´í„°ë² ì´ìŠ¤ì—ìˆëŠ” ë°©ëŒ€í•œ ì–‘ì˜ ì‚¬ìš©ì ë°ì´í„°ë¡œ ì¸í•´ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ê³  ìˆìœ¼ë©° ì›¹ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ì ë°ì´í„°ì˜ ì–‘ì´ ì¦ê°€í•¨ì— ë”°ë¼ ë” ë§ì€ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.

ì¶”ì²œ ì‹œìŠ¤í…œì˜ í™•ì¥ ì„±ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¬ ìˆ˜ìˆëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì´ ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ëŠ” CF ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì„ìœ„í•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí•˜ê³  ì‹¤í—˜ì ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” í•­ëª© ê¸°ë°˜ ê¸°ìˆ ì´ CF ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ë¡œ í™•ì¥í•˜ëŠ” ë™ì‹œì— ê³ í’ˆì§ˆ ê¶Œì¥ ì‚¬í•­ì„ ìƒì„± í•  ìˆ˜ ìˆë‹¤ëŠ” ì•½ì†ì„ ìœ ì§€í•˜ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


